{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "DJ Adams",
  "language": "en",
  "home_page_url": "https://qmacro.org/",
  "feed_url": "https://example.com/feed/feed.json",
  "description": "Reserving the right to be wrong.",
  "author": {
    "name": "DJ Adams",
    "url": "https://qmacro.org/about/"
  },
  "items": [{
      "id": "https://qmacro.org/blog/posts/2021-12-09-setting-up-hadolint---a-dockerfile-linter/",
      "url": "https://qmacro.org/blog/posts/2021-12-09-setting-up-hadolint---a-dockerfile-linter/",
      "title": "Setting up hadolint - a Dockerfile linter",
      "content_html": "<p><em>Having something to help me write better Dockerfiles is useful. Here's what I did to set up a Dockerfile linter in my development environment.</em></p>\n<p>I'm writing more Dockerfiles, not least because I'm using a <a href=\"https://github.com/qmacro/dotfiles/tree/main/devcontainer\">development container</a> for 95% of my daily work, but also because the dockerisation of tools and environments appeals to me greatly. I came across <a href=\"https://github.com/hadolint/hadolint\">hadolint</a> which is a Dockerfile linter written in Haskell (hence the name, I guess).</p>\n<p>I'm a <a href=\"https://qmacro.org/2021/05/19/supporting-developers-with-sponsorship/\">big fan</a> of <a href=\"https://github.com/koalaman/shellcheck\">shellcheck</a> (see the post <a href=\"https://qmacro.org/2020/10/05/improving-my-shell-scripting/\">Improving my shell scripting</a>) and the structured way it communicates the information, warning and error messages with codes in a standard format (SCnnnn). So I was immediately attracted to <code>hadolint</code> in two ways - first, that it referenced <a href=\"https://github.com/koalaman/shellcheck\">shellcheck</a>, but mostly because it implemented and managed its own <a href=\"https://github.com/hadolint/hadolint#rules\">rules</a> in a very similar way - each of them with a code in a standard format (DLnnnn) and individually documented too, just like <code>shellcheck</code>.</p>\n<p>There are different points in your workflow that you can integrate such a tool - these are nicely described in a dedicated <a href=\"https://github.com/hadolint/hadolint/blob/master/docs/INTEGRATION.md\">integration</a> page. I wanted to have the linting happen in my editor, and am already using the <a href=\"https://github.com/dense-analysis/ale\">Asynchronous Linting Engine</a> so it was quite straightforward. Here's what I did:</p>\n<h2 id=\"install-hadolint\" tabindex=\"-1\">Install hadolint <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-12-09-setting-up-hadolint---a-dockerfile-linter/#install-hadolint\" aria-hidden=\"true\">#</a></h2>\n<p>I installed <code>hadolint</code> with Homebrew on my macOS host, and by pulling down the latest binary in the Dockerfile for my development container. It's a single executable, which is quite neat. I may look into using <code>hadolint</code> as a Docker image instead, although I didn't at this stage because of various reasons (mostly involving a recently introduced security policy on this work laptop that automatically stops the SSH daemon, rendering the <a href=\"https://qmacro.org/2021/06/12/remote-access-to-docker-on-my-synology-nas/\">secure remote access to my Docker engine</a> useless. But that's a story for another time).</p>\n<h2 id=\"set-up-hadolint-as-a-linter\" tabindex=\"-1\">Set up hadolint as a linter <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-12-09-setting-up-hadolint---a-dockerfile-linter/#set-up-hadolint-as-a-linter\" aria-hidden=\"true\">#</a></h2>\n<p>I already use various tools for linting my content - <code>shellcheck</code>, <code>yamllint</code> and <a href=\"https://qmacro.org/2021/05/14/notes-on-markdown-linting-part-2/\">markdownlint</a>, and have configuration set up for that, so I just <a href=\"https://github.com/qmacro/dotfiles/commit/a2a3439956dc0eba7b6e8bc2e44eec0411284110\">added <code>hadolint</code> to the list</a>, which now looks like this:</p>\n<pre class=\"language-vim\"><code class=\"language-vim\"><span class=\"token keyword\">let</span> g<span class=\"token punctuation\">:</span>ale_linters <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><br />      \\ <span class=\"token string\">'sh'</span><span class=\"token punctuation\">:</span>         <span class=\"token punctuation\">[</span><span class=\"token string\">'shellcheck'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'language_server'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><br />      \\ <span class=\"token string\">'yaml'</span><span class=\"token punctuation\">:</span>       <span class=\"token punctuation\">[</span><span class=\"token string\">'yamllint'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><br />      \\ <span class=\"token string\">'markdown'</span><span class=\"token punctuation\">:</span>   <span class=\"token punctuation\">[</span><span class=\"token string\">'markdownlint'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><br />      \\ <span class=\"token string\">'dockerfile'</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'hadolint'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><br />      \\ <span class=\"token punctuation\">}</span></code></pre>\n<p>Because I sometimes create Dockerfiles with different names, I also added a new section to my Vim configuration telling it that these files are also to be treated as Dockerfiles:</p>\n<pre class=\"language-vim\"><code class=\"language-vim\">augroup filetypes<br />  au<span class=\"token operator\">!</span><br />  <span class=\"token builtin\">autocmd</span> BufNewFile<span class=\"token punctuation\">,</span>BufRead Dockerfile<span class=\"token operator\">*</span> <span class=\"token keyword\">set</span> <span class=\"token keyword\">filetype</span><span class=\"token operator\">=</span>dockerfile<br />augroup END</code></pre>\n<p>Now I get lovely warnings and errors in the left hand column so that I can improve:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/12/hadolint.png\" alt=\"warnings and errors in my editor from hadolint\" /></p>\n<p>In case you're wondering, the message details are shown at the bottom of my editor when I select the lines, and they are (in order):</p>\n<ul>\n<li><a href=\"https://github.com/hadolint/hadolint/wiki/DL3007\">DL3007</a> Using latest is prone to errors if the image will ever update. Pin the version explicitly to a release tag.</li>\n<li><a href=\"https://github.com/hadolint/hadolint/wiki/DL4000\">DL4000</a> MAINTAINER is deprecated.</li>\n<li><a href=\"https://github.com/hadolint/hadolint/wiki/DL4006\">DL4006</a> Set the SHELL option -o pipefail before RUN with a pipe in</li>\n</ul>\n<p>All very helpful - thanks <code>hadolint</code>!</p>\n",
      "date_published": "2021-12-09T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/",
      "url": "https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/",
      "title": "Learning by rewriting - bash, jq and fzf details",
      "content_html": "<p><em>One of the ways I learn is by reading and sometimes rewriting other people's scripts. Here I learn more about <code>jq</code> by rewriting a friend's password CLI script.</em></p>\n<p>My friend <a href=\"https://twitter.com/ceedee666\">Christian Drumm</a> published a nice post this week on <a href=\"https://www.drumm.sh/blog/bw-cli\">Adapting the Bitwarden CLI with Shell Scripting</a>, where he shared a script he wrote to conveniently grab passwords into his paste buffer at the command line.</p>\n<p>It's a good read and contains some nice CLI animations too. In the summary, Christian remarks that there may be some areas for improvement. I don't know about that, and I'm certainly no &quot;shell scripting magician&quot; but I thought I'd have a go at modifying the script to perhaps introduce some further Bash shell, <code>jq</code> and <code>fzf</code> features to dig into.</p>\n<h2 id=\"emulating-the-cli\" tabindex=\"-1\">Emulating the CLI <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/#emulating-the-cli\" aria-hidden=\"true\">#</a></h2>\n<p>I don't have Bitwarden, so I created a quick &quot;database&quot; of login information that took the form of what the Bitwarden CLI <code>bw</code> produced. First, then, is the contents of the <code>items.json</code> file:</p>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">[</span><br />  <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"E45 S4HANA 2020 Sandbox\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"username\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"e45user\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"password\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"sappass\"</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />  <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"space user\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"username\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"spaceuser\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"password\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"in space\"</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />  <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"foo\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"username\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"foouser\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"password\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"foopass\"</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />  <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"bar\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"username\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"baruser\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"password\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"sekrit!\"</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />  <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"baz\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span> <span class=\"token property\">\"username\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"bazuser\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"password\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"hunter2\"</span> <span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">]</span></code></pre>\n<p>Then I needed to emulate the <code>bw list items --search</code> command that Christian uses to search for an entry. As far as I can tell, it returns an array, regardless of whether a single entry is found, or more than one. I'm also assuming it returns an empty array if nothing is found, but that's less important here as you'll see.</p>\n<p>I did this by creating a script <code>bw-list-items-search</code> which looks like this:</p>\n<pre><code>#!/usr/bin/env bash\n\n# Emulates 'bw list items --search $1'\n\njq --arg name &quot;$1&quot; 'map(select(.name | test($name; &quot;i&quot;)))' ./items.json\n</code></pre>\n<p>Perhaps unironically I'm using <code>jq</code> to emulate the behaviour, because the data being searched is a JSON array (in <code>items.json</code>). I map over the entries in the array, and use the <a href=\"https://stedolan.github.io/jq/manual/#select(boolean_expression)\"><code>select</code> function</a> to return only those entries that satisfy the boolean expression passed to it:</p>\n<pre class=\"language-jq\"><code class=\"language-jq\"><span class=\"token punctuation\">.</span>name <span class=\"token operator pipe\">|</span> <span class=\"token c-style-function function\">test</span><span class=\"token punctuation\">(</span><span class=\"token variable\">$name</span><span class=\"token punctuation\">;</span> <span class=\"token string\">\"i\"</span><span class=\"token punctuation\">)</span></code></pre>\n<p>This pipes the value of the <code>name</code> property (e.g. &quot;E45 S4HANA 2020 Sandbox&quot;, &quot;space user&quot;, &quot;foo&quot; etc) into the <a href=\"https://stedolan.github.io/jq/manual/#test(val),test(regex;flags)\"><code>test</code> function</a> which can take a regular expression, along with one or more flags if required.</p>\n<p>Here, we're just taking the value passed into the script, via the argument that was passed to the <code>jq</code> invocation with <code>--arg name &quot;$1&quot;</code>. This is then available within the <code>jq</code> script as the binding <code>$name</code>. The second parameter supplied here, <code>&quot;i&quot;</code>, is the &quot;case insensitive match&quot; flag.</p>\n<p>The result means that I can emulate what I think <code>bw list items --search</code> does:</p>\n<pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token punctuation\">;</span> ./bw-list-items-search e45<br /><span class=\"token punctuation\">[</span><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token string\">\"name\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"E45 S4HANA 2020 Sandbox\"</span>,<br />    <span class=\"token string\">\"login\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">{</span><br />      <span class=\"token string\">\"username\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"e45user\"</span>,<br />      <span class=\"token string\">\"password\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"sappass\"</span><br />    <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">]</span></code></pre>\n<p>Here's an example of where more than one result is found:</p>\n<pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token punctuation\">;</span> ./bw-list-items-search ba<br /><span class=\"token punctuation\">[</span><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token string\">\"name\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"bar\"</span>,<br />    <span class=\"token string\">\"login\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">{</span><br />      <span class=\"token string\">\"username\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"baruser\"</span>,<br />      <span class=\"token string\">\"password\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"sekrit!\"</span><br />    <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">}</span>,<br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token string\">\"name\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"baz\"</span>,<br />    <span class=\"token string\">\"login\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">{</span><br />      <span class=\"token string\">\"username\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"bazuser\"</span>,<br />      <span class=\"token string\">\"password\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"hunter2\"</span><br />    <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">]</span></code></pre>\n<h2 id=\"the-main-script\" tabindex=\"-1\">The main script <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/#the-main-script\" aria-hidden=\"true\">#</a></h2>\n<p>Now I could turn my attention to the main script. Here it is in its entirety; I'll describe it section by section.</p>\n<pre><code>#!/usr/bin/env bash\n\nset -e\n\npbcopy() { true; }\n\ncopy_uname_and_passwd() {\n\n  local login=$1\n\n  echo &quot;&gt; Copying Username&quot;\n  jq -r '.username' &lt;&lt;&lt; &quot;$login&quot;\n\n  echo &quot;&gt; Press any key to copy password...&quot;\n  read\n  echo &quot;&gt; Copying Password&quot;\n  jq -r '.password' &lt;&lt;&lt; &quot;$login&quot;\n\n}\n\nmain() {\n\n  local searchterm=$1\n  local selection logins\n  logins=&quot;$(./bw-list-items-search $searchterm)&quot;\n\n  selection=&quot;$(jq -r '.[] | &quot;\\(.name)\\t\\(.login)&quot;' &lt;&lt;&lt; &quot;$logins&quot; \\\n    | fzf --reverse --with-nth=1 --delimiter=&quot;\\t&quot; --select-1 --exit-0\n  )&quot;\n\n  [[ -n $selection ]] \\\n    &amp;&amp; echo &quot;Name: ${selection%%$'\\t'*}&quot; \\\n    &amp;&amp; copy_uname_and_passwd &quot;${selection#*$'\\t'}&quot;\n\n}\n\nmain &quot;$@&quot;\n</code></pre>\n<h3 id=\"overall-structure-and-the-main-function\" tabindex=\"-1\">Overall structure and the main function <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/#overall-structure-and-the-main-function\" aria-hidden=\"true\">#</a></h3>\n<p>For the last few months, my preference for laying out non-trivial scripts has been to use the approach that one often finds in other languages, and that is to define a main function, and right at the bottom, call that to start things off.</p>\n<p>This call is <code>main &quot;$@&quot;</code> which just passes on any and all values that were specified in the script's invocation - they're available in the special parameter <code>$@</code> which &quot;expands to the positional parameters, starting from one&quot; (see <a href=\"https://tiswww.case.edu/php/chet/bash/bashref.html#Special-Parameters\">Special Parameters</a>).</p>\n<h3 id=\"the-main-function\" tabindex=\"-1\">The main function <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/#the-main-function\" aria-hidden=\"true\">#</a></h3>\n<p>I like to qualify my variables, so use <code>local</code> here, which is a synonym for <code>declare</code>. I wrote about this in <a href=\"https://qmacro.org/autodidactics/2020/10/08/understanding-declare/\">Understanding declare</a> in case you want to dig in further.</p>\n<p>Because I have my emulator earlier, I can make almost the same-shaped call to the Bitwarden CLI, passing what was specified in <code>searchterm</code> and retrieving the results (a JSON array) in the <code>logins</code> variable.</p>\n<p>Next comes perhaps the most involved part of the script, which results in a value being stored in the <code>selection</code> variable (if nothing is selected or available, then this will be empty, which we'll deal with too).</p>\n<p><strong>Determining the selection part 1 - with <code>jq</code></strong></p>\n<p>The value for <code>selection</code> is determined from a combination of <code>jq</code> and <code>fzf</code>, which are also the two commands that Christian uses.</p>\n<p>This is the invocation:</p>\n<pre><code>jq -r '.[] | &quot;\\(.name)\\t\\(.login)&quot;' &lt;&lt;&lt; &quot;$logins&quot; \\\n    | fzf --reverse --with-nth=1 --delimiter=&quot;\\t&quot; --select-1 --exit-0\n</code></pre>\n<p>The first thing to notice is that I'm using <code>&lt;&lt;&lt;</code> which is a <a href=\"https://tldp.org/LDP/abs/html/x17837.html\">here string</a> - it's like a <a href=\"https://tldp.org/LDP/abs/html/here-docs.html\">here document</a>, but it's just the variable that gets expanded and fed to the STDIN of the command. This means that whatever is in <code>logins</code> gets expanded and passed to the STDIN of <code>jq</code>.</p>\n<p>Given the emulation of the Bitwarden CLI above, a value that might be in <code>logins</code> looks like this:</p>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">[</span><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"bar\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />      <span class=\"token property\">\"username\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"baruser\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"password\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"sekrit!\"</span><br />    <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />  <span class=\"token punctuation\">{</span><br />    <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"baz\"</span><span class=\"token punctuation\">,</span><br />    <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />      <span class=\"token property\">\"username\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"bazuser\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"password\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"hunter2\"</span><br />    <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">}</span><br /><span class=\"token punctuation\">]</span></code></pre>\n<p>Let's look at the <code>jq</code> script now, which is this:</p>\n<pre class=\"language-jq\"><code class=\"language-jq\"><span class=\"token punctuation\">.</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span> <span class=\"token operator pipe\">|</span> <span class=\"token string\">\"<span class=\"token interpolation\"><span class=\"token punctuation\">\\(</span><span class=\"token content\"><span class=\"token punctuation\">.</span>name</span><span class=\"token punctuation\">)</span></span>\\t<span class=\"token interpolation\"><span class=\"token punctuation\">\\(</span><span class=\"token content\"><span class=\"token punctuation\">.</span>login</span><span class=\"token punctuation\">)</span></span>\"</span></code></pre>\n<p>This iterates over the items passed in (i.e. it will process the first object containing the details for &quot;bar&quot; and then the second object containing the details for &quot;baz&quot;) and pipes them into the creation of a literal string (enclosed in double quotes). This literal string is two values separated with a tab character (<code>\\t</code>) ... but those values are the values of the respective properties, via <code>jq</code>'s <a href=\"https://stedolan.github.io/jq/manual/#Stringinterpolation-(foo\">string interpolation</a>).</p>\n<p>It's worth noting that the value of <code>.name</code> is a scalar, e.g. &quot;bar&quot;, but the value of <code>.login</code> is actually an object:</p>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"username\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"baruser\"</span><span class=\"token punctuation\">,</span><br />  <span class=\"token property\">\"password\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"sekrit!\"</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>but this gets turned into a string. If &quot;bar&quot; is selected, then the value in <code>selection</code> will be:</p>\n<pre><code>bar     {&quot;username&quot;:&quot;baruser&quot;,&quot;password&quot;:&quot;sekrit!&quot;}\n</code></pre>\n<p>where the whitespace between the name &quot;bar&quot; and the rest of the line is a tab character.</p>\n<p>So given the two values (for &quot;bar&quot; and &quot;baz&quot;) above which would have been extracted for the search string &quot;ba&quot;, the following would be produced by the <code>jq</code> invocation:</p>\n<pre><code>bar     {&quot;username&quot;:&quot;baruser&quot;,&quot;password&quot;:&quot;sekrit!&quot;}\nbaz     {&quot;username&quot;:&quot;bazuser&quot;,&quot;password&quot;:&quot;hunter2&quot;}\n</code></pre>\n<p>Note that the <code>-r</code> option is supplied to <code>jq</code> to produce this raw output.</p>\n<p><strong>Determining the selection part 2 - with <code>fzf</code></strong></p>\n<p>This is then passed to <code>fzf</code>, which is passed a few more options than we saw with Christian's script. Taking them one at a time:</p>\n<ul>\n<li><code>--reverse</code> - this is the same as Christian and is a layout option that causes the selection to be displayed from the top of the screen.</li>\n<li><code>--delimiter=&quot;\\t&quot;</code> - this tells <code>fzf</code> how the input fields are delimited, and as we're using a tab character to separate the name and login information, we need to tell <code>fzf</code> (using just spaces would give us issues with spaces in the values of the names).</li>\n<li><code>--with-nth=1</code> - this says &quot;only use the value of the first field in the selection list&quot;, where the fields are delimited as instructed (with the tab character here). This means that only the value of the &quot;name&quot; is presented, not the &quot;login&quot; (username and password) details.</li>\n<li><code>--select-1</code> - this tells <code>fzf</code> that if there's only one item in the selection anyway, just automatically select it and don't show any selection dialogue.</li>\n<li><code>--exit-0</code> - this tells <code>fzf</code> to just end if there's nothing to select from at all (which would be the case if the invocation to <code>bw list items --search</code> returned nothing, i.e. an empty array).</li>\n</ul>\n<p>Here's what the selection looks like if no search string is specified, i.e. it's a presentation of all the possible names:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/08/fzf-name-selection.png\" alt=\"selection in fzf\" /></p>\n<p>Once we're done with determining the selection, we check to see that there is actually a value in <code>selection</code> and proceed to first show the name and then to call the <code>copy_uname_and_passwd</code> function.</p>\n<p><strong>Displaying the name and extracting the login details</strong></p>\n<p>It's worth highlighting that while <code>fzf</code> only <em>presents</em> the names in the selection list, it will <em>return</em> the entire line that was selected, which is what we want. In other words, given the selection in the screenshot above, if the name &quot;E45 S4HANA 2020 Sandbox&quot; is chosen, then <code>fzf</code> will emit this to STDOUT:</p>\n<pre><code>E45 S4HANA 2020 Sandbox {&quot;username&quot;:&quot;e45user&quot;,&quot;password&quot;:&quot;sappass&quot;}\n</code></pre>\n<p>(again, remember that there's a tab character between the name &quot;E45 S4HANA 2020 Sandbox&quot; and the JSON object with the login details).</p>\n<p>So to just print the name, we can use <a href=\"https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html\">shell parameter expansion</a> to pick out the part we want. The <code>${parameter%%word}</code> form is appropriate here; this will remove anything with longest matching pattern first.</p>\n<p>In other words, the expression <code>${selection%%$'\\t'*}</code> means:</p>\n<ul>\n<li>take the value of the <code>selection</code> variable</li>\n<li>look at the <em>trailing</em> portion of that value</li>\n<li>find the <em>longest</em> match of the pattern <code>$'\\t'*</code></li>\n<li>and remove it</li>\n</ul>\n<p>The <code>$'...'</code> way of quoting a string allows us to use special characters such as tab (<code>\\t</code>) safely. The <code>*</code> means &quot;anything&quot;. So the pattern is &quot;a tab character and whatever follows it, if anything&quot;.</p>\n<p>So if the value of <code>selection</code> is:</p>\n<pre><code>E45 S4HANA 2020 Sandbox {&quot;username&quot;:&quot;e45user&quot;,&quot;password&quot;:&quot;sappass&quot;}\n</code></pre>\n<p>then this expression will yield:</p>\n<pre><code>E45 S4HANA 2020 Sandbox\n</code></pre>\n<p>The expression in the next line, where we invoke the <code>copy_uname_and_passwd</code> function, is <code>${selection#*$'\\t'}</code> which is similar. It means:</p>\n<ul>\n<li>take the value of the <code>selection</code> variable</li>\n<li>look at the <em>beginning</em> portion of that value</li>\n<li>find the <em>shortest</em> match of the pattern <code>*$'\\t'</code></li>\n<li>and remove it</li>\n</ul>\n<p>This pattern, then, is &quot;anything, up to and including a tab character&quot;.</p>\n<p>Given the same value as above, this expression will yield:</p>\n<pre><code>{&quot;username&quot;:&quot;e45user&quot;,&quot;password&quot;:&quot;sappass&quot;}\n</code></pre>\n<h3 id=\"the-copy_uname_and_passwd-function\" tabindex=\"-1\">The copy_uname_and_passwd function <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/#the-copy_uname_and_passwd-function\" aria-hidden=\"true\">#</a></h3>\n<p>This is very similar to Christian's original script, except that we can use a &quot;here string&quot; again to pass the value of the <code>login</code> variable to <code>jq</code> each time. Given what we know from the <code>main</code> function, this value will be something like this:</p>\n<pre><code>{&quot;username&quot;:&quot;e45user&quot;,&quot;password&quot;:&quot;sappass&quot;}\n</code></pre>\n<p>which makes for a simpler extraction of the values we want (from the <code>username</code> and <code>password</code> properties).</p>\n<h3 id=\"the-pbcopy-function\" tabindex=\"-1\">The pbcopy function <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/#the-pbcopy-function\" aria-hidden=\"true\">#</a></h3>\n<p>While my main machine is a macOS device, I'm working in a (Linux based) dev container and therefore don't have access right now to the <code>pbcopy</code> command. As I wanted to leave calls to it in the script to reflect where it originally was, this function that does nothing will do the trick.</p>\n<h2 id=\"wrapping-up\" tabindex=\"-1\">Wrapping up <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-08-26-learning-by-rewriting/#wrapping-up\" aria-hidden=\"true\">#</a></h2>\n<p>There's always more to learn about Bash scripting and the tools we have at our disposal. And to use one of the sayings from the wonderful Perl community - TMOWTDI - &quot;there's more than one way to do it&quot;. I'm sure you can come up with some alternatives too, and some improvements on what I've written.</p>\n<p>Keep on learning and sharing.</p>\n",
      "date_published": "2021-08-26T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-07-21-unix-tooling---join,-don&amp;#39;t-extend/",
      "url": "https://qmacro.org/blog/posts/2021-07-21-unix-tooling---join,-don&amp;#39;t-extend/",
      "title": "Unix tooling - join, don&#39;t extend",
      "content_html": "<p><em>Reading a paper from 1984 has helped crystallise an important axiom in Unix tool design, so much so that I found myself referring to it today when scripting.</em></p>\n<p>Rob Pike and Brian Kernighan authored a paper in 1984 titled &quot;<a href=\"https://nymity.ch/sybilhunting/pdf/Pike1983a.pdf\">Program design in the UNIX environment</a>&quot;. In it, they explore the difference between adding features to existing programs, and achieving the same effect through connecting programs together.</p>\n<p>It's not unusual to read about the UNIX philosophy of &quot;small programs, loosely joined&quot;, about the power of small, single-responsibility programs doing one thing and doing it well, used together to form something greater than merely the sum of its parts.</p>\n<p>What really jumped out at me was the thinly veiled scorn poured onto some design decisions relating to the Berkeley distribution version of the humble <code>ls</code> command, design decisions ultimately based on a form of narrow thinking, rather than consideration of the UNIX environment - particularly the shell and the user space programs - as being the most important &quot;meta tool&quot;.</p>\n<p>I won't try to summarise what that paper says about those extensions to <code>ls</code>, I would rather encourage you to go and read the paper yourself - it's only 7 pages. Instead, I'll relate a small example from today of how the paper has helped me remember the difference between focusing on a single tool to the detriment of the rest of the tools in the environment, and thinking about the entire environment as a single entity.</p>\n<p>I have a small script, <a href=\"https://github.com/qmacro/dotfiles/blob/master/scripts/skv\"><code>skv</code></a> which is short for &quot;service key value&quot;, which returns a value from a property within a JSON dataset representing a (Cloud Foundry) service key. In retrospect, calling it <code>skv</code> was a little short-sighted, because it would work on any JSON dataset, not just service keys. I guess that's another example of (not) thinking about the environment as the real &quot;meta tool&quot;. But I digress.</p>\n<p>Today I've been using <code>skv</code> to grab OAuth-related values (client ID, client secret, and so on) from a JSON dataset and use them to construct URLs to follow an OAuth authorisation code flow. Some of those values are put into the query string of the URLs I'm constructing, and so I need to be careful to URL encode them.</p>\n<p>My first reaction was to go into the source code of the <code>skv</code> script to add a switch, say, <code>-u</code>, along with the corresponding logic, so that I could ask, when invoking <code>skv</code>, that the value be returned URL encoded.</p>\n<p>I'm happy to say that a second after having this reaction, I felt almost horrified that I was about to do exactly what those Berkeley authors did, and add an unnecessary switch to a single program. I already have <code>urlencode</code> available to me in my environment, so to get a URL encoded value from the JSON dataset, I'd just have to do something like this:</p>\n<pre><code>skv uaa.clientid | urlencode\n</code></pre>\n<p>This is only a trivial example, and there's a difference because here I'm just stringing together commands in Bash scripts, but I think the principle still holds here. I feel that this approach is embracing the UNIX approach described in the paper.</p>\n<p>What's more, there was nothing stopping me encapsulating the pipeline-based use of these two simple tools (<code>skv</code> and <code>urlencode</code>) in a little script <code>skvu</code>, that I could use to save some keystrokes:</p>\n<pre><code>urlencode &quot;$(skv &quot;$*&quot;)&quot;\n</code></pre>\n<p>In fact here I've re-jigged how these two commands work together, as the version of <code>urlencode</code> I settled on works on a value passed as an argument, rather than passed via STDIN. But the beauty of the shell means that this just means I have to express my intention in a way that echoes that approach.</p>\n<p>Anyway, that's all I wanted to share here - it's not a crazily interesting and little-understood corner of the UNIX church that I'm discovering here, merely the delight in something resonating with me to the extent that the strong reverberations carried through to something that I was actually doing, without consciously thinking of what I'd read.</p>\n<p>I hope you enjoy the paper as much as I did!</p>\n",
      "date_published": "2021-07-21T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-07-20-early-thoughts-on-warp/",
      "url": "https://qmacro.org/blog/posts/2021-07-20-early-thoughts-on-warp/",
      "title": "Early thoughts on Warp",
      "content_html": "<p><em>Here are some very early thoughts on Warp, the &quot;pro terminal designed for everyday use&quot;.</em></p>\n<p>Today I was <a href=\"https://twitter.com/Fidschenheimer/status/1417416096131518469\">pointed</a> in Warp's direction on Twitter by <a href=\"https://twitter.com/Fidschenheimer\">Christian Pfisterer</a> and <a href=\"https://twitter.com/ceedee666\">Christian Drumm</a>.</p>\n<p>To quote <a href=\"https://www.warp.dev/\">Warp's website</a>:</p>\n<blockquote>\n<p>Warp is a blazingly fast, Rust-based terminal that makes you and your team more productive at coding and DevOps.</p>\n</blockquote>\n<p>It's a fascinating venture, for many reasons. While the team is not looking to reinvent the entire terminal, a lot of what they describe feels &quot;foreign&quot; to me, as a long time terminal user (who <a href=\"https://qmacro.org/2020/11/03/computer-unit-1979/\">started out</a> on a paper-based Superterm Data Communications Terminal hooked up to a PDP-11). I've read the <a href=\"https://blog.warp.dev/how-warp-works/\">How it works</a> post, which is great. Here are some random thoughts on that, and also on the <a href=\"https://youtu.be/X0LzWAVlOC0\">Warp beta welcome</a> video.</p>\n<h2 id=\"speed\" tabindex=\"-1\">Speed <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-07-20-early-thoughts-on-warp/#speed\" aria-hidden=\"true\">#</a></h2>\n<p>Warp is designed from the outset to be fast; written in Rust (a language designed in part with a major focus on performance) and using the GPU for rendering (which, to be fair, other terminal programs also do, such as my current terminal program of choice, <a href=\"https://sw.kovidgoyal.net/kitty/\">kitty</a>).</p>\n<p>What struck me is how far away my brain is from the sort of speed that this team is talking about; while I guess I still about terminal speed in terms of baud, and based on characters, the measurement for Warp is in frames per second; not only that, it's in the early gaming ballpark of 60fps. It feels a little odd thinking about a terminal in those terms.</p>\n<h2 id=\"input-affordances\" tabindex=\"-1\">Input affordances <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-07-20-early-thoughts-on-warp/#input-affordances\" aria-hidden=\"true\">#</a></h2>\n<p>The <a href=\"https://docs.warp.dev/features/the-input-editor\">input editor</a> is effectively reinvented. I'm in two minds about this; moreover, one of those minds is awash with ignorance and uncertainty. First and most importantly, the editing capabilities on command lines today, at least in popular shells such as Bash and ZSH, are very advanced already. I'm not talking about what perhaps most people use - the default Emacs based editing facilities, which I think the demo is comparing Warp to - but the Vi based ones.</p>\n<p>While I do like the idea of being able to more comfortably edit multiple lines, the other features feel rather redundant. With a <a href=\"https://github.com/qmacro/dotfiles/blob/master/bashrc.d/00-shell.sh#L1\">simple</a> <code>set -o vi</code> I am in total control of how I edit, fix, rearrange and generally prepare my input. Very powerful.</p>\n<p>The other mind is wondering about how this translates to remote sessions. The beauty of standard tools and shell facilities means that I can have exactly the same experience whether I'm local, or remote, via an ssh-based connection, to a machine elsewhere on the network, or to a container in a Kubernetes cluster.</p>\n<p>Will the input editor allow this to happen in these remote contexts too? The <a href=\"https://docs.warp.dev/features/ssh\">SSH</a> section does seem to say that this is possible, but I'm also wondering about whether that is also valid for ssh sessions within <a href=\"https://github.com/tmux/tmux/wiki\">tmux</a> panes?</p>\n<h2 id=\"blocks-of-commands\" tabindex=\"-1\">Blocks of commands <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-07-20-early-thoughts-on-warp/#blocks-of-commands\" aria-hidden=\"true\">#</a></h2>\n<p>Another feature of the Warp terminal, very nicely demonstrated in the video, is the concept of <a href=\"https://docs.warp.dev/features/blocks\">blocks</a>. This makes a lot of sense to me, and I can imagine already how it will help me visually move up and down examining and working with previous commands and their output.</p>\n<p>What worries me slightly is that it looks, at least from the demo, that I'll have to use the mouse if I want to scroll further up, via the &quot;snackbar&quot; (I'm not sure why it's called that, chalk another item down to my ignorance). I wonder if I'll be able to use the terminal as a terminal (yes, that's deliberately provocative) and keep my hands where they belong - on the keyboard?</p>\n<h2 id=\"the-future-is-terminal\" tabindex=\"-1\">The future is terminal <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-07-20-early-thoughts-on-warp/#the-future-is-terminal\" aria-hidden=\"true\">#</a></h2>\n<p>Whatever the answers to these questions turn out to be (and please note that I've not seen or tried out Warp for myself yet - I've added myself to the list requesting beta access when it's available), there's one thing that I'm very happy to see.</p>\n<p>And that's fresh thinking and energy going into what I think is one of the most misunderstood superpowers of today's computing space. So whatever this team comes up with, it's an automatic thumbs up for me. I may come to enjoy all Warp's features, or I may only like some of them. But I love the focus and <a href=\"https://www.warp.dev/about-us\">brain power</a> that's going into Warp.</p>\n<p>Here's a picture of Warp, from the very interesting <a href=\"https://blog.warp.dev/how-warp-works/\">How Warp works</a> article I mentioned earlier.</p>\n<p><img src=\"https://blog.warp.dev/blog/img/2021/07/pasted-image-0-2.png\" alt=\"A picture of Warp\" /></p>\n<p>Make it so!</p>\n",
      "date_published": "2021-07-20T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-06-14-the-apc-smt750ic-ups-works-with-the-synology-nas-ds1621+/",
      "url": "https://qmacro.org/blog/posts/2021-06-14-the-apc-smt750ic-ups-works-with-the-synology-nas-ds1621+/",
      "title": "The APC SMT750IC UPS works with the Synology NAS DS1621+",
      "content_html": "<p><em>I've successfully configured this setup, and the USB connection from the UPS to the NAS does indeed work to tell the NAS to shut down.</em></p>\n<p>TL;DR - My Synology DS1621+ NAS recognises the USB-connected APC SMT750IC UPS and will shut itself down on signals sent from it.</p>\n<p>Since buying my Synology NAS DS1621+ a few weeks ago, we've had one power outage in the village. I'd been musing on the idea of getting a UPS for the NAS, and this event helped me come to a decision (a little late, perhaps, but there you go). It took me longer than it should have done to work out which UPS might be applicable and compatible. I couldn't find definitive confirmation that the UPS I was looking at was going to work with the NAS; in particular, I wanted to be as sure as I could that the USB connection would indeed be recognised by the NAS, which would receive power event signals and shut itself down as appropriate when the UPS had to switch to battery power.</p>\n<p>Synology maintain a <a href=\"https://www.synology.com/en-us/compatibility?search_by=products&amp;model=DS1621%2B&amp;category=upses&amp;p=1&amp;change_log_p=1\">Compatibility List</a> and the APC Smart-UPS SMT750IC is indeed in there, with the value &quot;Vendor Recommended&quot; in the &quot;Tested by&quot; column. Reading around, I got the impression that this indeed meant what I suspected it meant, i.e. Synology themselves hadn't tested it, but instead were relying on APC to tell them. While I had no reason to doubt APC, I am fond of the proverb <em>Доверяй, но проверяй</em> (<a href=\"https://en.wikipedia.org/wiki/Trust,_but_verify\">Trust, but verify</a>) and needed more solid evidence, especially before splashing out the <a href=\"https://www.amazon.co.uk/gp/product/B07DM6BPM2/\">£300+</a> on the device (shipping it back might also have been a pain, due to its extreme weight).</p>\n<p>I'd seen a few bits and pieces about the SMT750IC model's predecessor, the SMT750I, and some evidence that folks were successfully using this older SMT750I model with their Synology NAS devices, including the USB-based shutdown flows. My research told me that the &quot;C&quot; suffix on the newer model represented a new cloud enabled feature, described in the blurb thus: &quot;APC SmartConnect is a proactive remote UPS cloud monitoring feature that is accessible from any internet connected device&quot;. I'd also seen some vague confirmation that alongside some minor performance improvements, this cloud feature was really the only difference.</p>\n<p>So it would seem reasonable to assume that the SMT750IC was going to be OK. But viewing the ports on the back of each device showed me that the USB connection was different (you can also see the green-coloured ethernet port on the SMT750IC relating to its &quot;cloud enabled&quot; feature):</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/06/smt-devices-rear.png\" alt=\"The backs of each of the SMT750I and SMT750IC\" /></p>\n<p>Was this USB port difference significant? It was hard to tell. Perhaps the USB port on the SMT750I was a type B for a reason? Had the USB support on the SMT750IC changed?</p>\n<p>Further research suggested that on the one hand, if the APC &quot;Powerchute&quot; software was supported by the UPS, it was likely to work with the NAS, mostly because of Synology's support for the <a href=\"https://networkupstools.org/\">Network UPS Tools (NUT) standard</a>. But then I read elsewhere that this standard had multiple implementations, so it wasn't a certainty by any means.</p>\n<p>In the end, I <a href=\"https://www.amazon.co.uk/ask/questions/Tx32WEPA58FDXDS/ref=ask_dp_dpmw_al_hza\">asked on the Amazon product page</a>, and also called their UK support centre. Both avenues resulted in a positive outcome - I got a positive reply from APC Customer Care and also from the user &quot;Pegasus&quot;, and the person on the phone also confirmed this.</p>\n<p>So if you're in the same situation as I was, perhaps this post will help.</p>\n<p>Here are some screenshots of when I tested the UPS and NAS, removing the power from the UPS so that it switched to battery mode.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/06/ups-recognised.png\" alt=\"The UPS settings on the NAS, showing the UPS is recognised via USB\" /></p>\n<p>The UPS settings on the NAS, showing the UPS is recognised via USB.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/06/cloud-status-power.png\" alt=\"The UPS's normal status showing via the &quot;cloud enabled&quot; feature on APC's website\" /></p>\n<p>The UPS's normal status showing via the &quot;cloud enabled&quot; feature on APC's website.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/06/ups-shutdown-display.jpeg\" alt=\"The UPS's front panel display shortly after I removed the power\" /></p>\n<p>The UPS's front panel display shortly after I removed the power.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/06/ups-alert-on-nas.png\" alt=\"The alert on the NAS when the UPS has switched to battery mode\" /></p>\n<p>The alert on the NAS when the UPS has switched to battery mode.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/06/cloud-status-battery.png\" alt=\"The UPS's warning status showing via the &quot;cloud enabled&quot; feature on APC's website when the UPS is in battery mode\" /></p>\n<p>The UPS's warning status showing via the &quot;cloud enabled&quot; feature on APC's website when the UPS is in battery mode.</p>\n",
      "date_published": "2021-06-14T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/",
      "url": "https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/",
      "title": "Remote access to Docker on my Synology NAS",
      "content_html": "<p><em>Here's what I did to enable remote access to the Docker install on my Synology NAS.</em></p>\n<p>This post describes the steps I took to set up remote access to Docker running on my NAS, in the simplest and &quot;smallest footprint&quot; possible way I could find. There are other approaches, but this is what I did. It was a little less obvious than one might have expected, because of the way the Docker service is hosted on the NAS's operating system, and I ended up having to read around (see the reading list at the end).</p>\n<h2 id=\"introduction\" tabindex=\"-1\">Introduction <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#introduction\" aria-hidden=\"true\">#</a></h2>\n<p>Having followed the container revolution for a while, I've become more and more enamoured with the idea of disposable workspaces, services and apps that can be instantly reified and leave no trace when they're gone. This was one of the reasons I opted for a Synology NAS, my first NAS device (see <a href=\"https://qmacro.org/2021/05/22/adding-a-drive-to-my-synology-nas/\">Adding a drive to my Synology NAS</a>), because it is to act not only as a storage device, but as a container server.</p>\n<p>The Docker experience out of the box with the NAS's operating system, DiskStation Manager (DSM), is very pleasant, via a graphical user interface. I've been very happy with the way it works, especially in the initial discovery phase.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/06/docker-gui.png\" alt=\"A screenshot of the Docker app installed on the Synology NAS, showing two running containers\" /></p>\n<p>But for this old mainframe and Unix dinosaur, a command line interface with access to a myriad remote servers is a much more appealing prospect, and the separation of client and server executables in Docker plays to the strengths of such a setup. So I wanted to use my Docker command line interface (CLI) <code>docker</code> to interact with the resources on the Synology NAS's Docker service. Not only for the sheer convenience, but also to be able to spin up CLIs and TUIs, as remote containers, and have seamless access to them from the comfort of my local machine's command line.</p>\n<h2 id=\"setup-steps\" tabindex=\"-1\">Setup steps <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#setup-steps\" aria-hidden=\"true\">#</a></h2>\n<p>Here's what I did, starting from the Docker package already installed and running on the NAS.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/06/docker-package.png\" alt=\"A screenshot of the Docker package installed on the NAS\" /></p>\n<p>From a command line perspective, this out of the box installation also gave me access to be able to run the <code>docker</code> client CLI while remotely logged into the NAS, but only as root, i.e. directly, or via <code>sudo</code> as shown in this example:</p>\n<pre><code>; ssh ds1621plus\nadministrator@ds1621plus:~$ sudo docker -v\nPassword:\nDocker version 20.10.3, build b35e731\nadministrator@ds1621plus:~$ sudo docker image ls\nREPOSITORY                     TAG       IMAGE ID       CREATED       SIZE\nhomeassistant/home-assistant   latest    832ca33fe14a   4 weeks ago   1.1GB\nlinuxserver/freshrss           latest    09ffc08f14fe   4 weeks ago   120MB\nadministrator@ds1621plus:~$\n</code></pre>\n<h3 id=\"allow-access-as-non-root-user\" tabindex=\"-1\">Allow access as non-root user <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#allow-access-as-non-root-user\" aria-hidden=\"true\">#</a></h3>\n<p>The first thing I wanted to do is to allow myself to run the <code>docker</code> CLI as a non-root user; in my case (as in many basic Synology NAS contexts) this is the as the <code>administrator</code> user.</p>\n<p>In the standard Docker <a href=\"https://docs.docker.com/engine/install/linux-postinstall/\">Post-installation steps for Linux</a>, there's a specific section for this: <a href=\"https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user\">Manage Docker as a non-root user</a>. However, due to the way that users and groups are managed in DSM, this specific approach didn't work; there was no <code>docker</code> group that had been created, to which the <code>administrator</code> user could be added, and manually adding the group wasn't the right approach either, not least because DSM doesn't sport a <code>groupadd</code> command.</p>\n<p>In fact, there are DSM specific commands for managing local users, groups, network settings and more. They all begin <code>syno</code> and are described in the <a href=\"https://global.download.synology.com/download/Document/Software/DeveloperGuide/Firmware/DSM/All/enu/Synology_DiskStation_Administration_CLI_Guide.pdf\">CLI Administrator Guide for Synology NAS</a>.</p>\n<p>So here's what I did. I'm a check-before-and-after kind of person, so some of these steps aren't essential, but they helped me to go carefully.</p>\n<h4 id=\"check-docker-group-doesn't-already-exist\" tabindex=\"-1\">Check docker group doesn't already exist <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#check-docker-group-doesn't-already-exist\" aria-hidden=\"true\">#</a></h4>\n<p>First, I wanted to check that I wasn't about to clobber any existing <code>docker</code> group:</p>\n<pre><code>administrator@ds1621plus:~$ grep -i docker /etc/group\nadministrator@ds1621plus:~$\n</code></pre>\n<p>Nope, no existing <code>docker</code> group, at least in the regular place.</p>\n<h4 id=\"add-the-docker-group-with-the-administrator-user-as-a-member\" tabindex=\"-1\">Add the docker group, with the administrator user as a member <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#add-the-docker-group-with-the-administrator-user-as-a-member\" aria-hidden=\"true\">#</a></h4>\n<p>Time to create the group then, using the DSM specific command; I specified the <code>administrator</code> user to be added directly, as I did it:</p>\n<pre><code>administrator@ds1621plus:~$ sudo synogroup --add docker administrator\nGroup Name: [docker]\nGroup Type: [AUTH_LOCAL]\nGroup ID:   [65538]\nGroup Members:\n0:[administrator]\n</code></pre>\n<p>Checking to see if the group was now listed in <code>/etc/group</code> confirmed that these DSM specific commands weren't doing anything out of the ordinary:</p>\n<pre><code>administrator@ds1621plus:~$ grep -i docker /etc/group\ndocker:x:65538:administrator\n</code></pre>\n<p>Great, the <code>docker</code> group now exists, with <code>administrator</code> as a member.</p>\n<h4 id=\"change-the-group-owner-of-the-docker-socket\" tabindex=\"-1\">Change the group owner of the Docker socket <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#change-the-group-owner-of-the-docker-socket\" aria-hidden=\"true\">#</a></h4>\n<p>The <a href=\"https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user\">Manage Docker as a non-root user</a> steps mentioned earlier showed that this is pretty much all one needs to do on a standard Docker-on-Linux install. However, there was an extra step needed on DSM, to actually assign to this new <code>docker</code> group access to the Unix socket that Docker uses.</p>\n<p>Before I did this, I wanted to see what the standard situation was:</p>\n<pre><code>administrator@ds1621plus:~$ ls -l /var/run/ | grep docker\ndrwx------ 8 root         root             200 Jun 10 17:40 docker\n-rw-r--r-- 1 root         root               5 Jun 10 17:40 docker.pid\nsrw-rw---- 1 root         root               0 Jun 10 17:40 docker.sock\n</code></pre>\n<p>The socket (<code>docker.sock</code>) in <code>/var/run/</code> was owned by <code>root</code> as user and <code>root</code> as group. This meant that no amount of membership of the <code>docker</code> group was going to get the <code>administrator</code> user any closer to being able to interact with Docker.</p>\n<p>So I changed the group ownership to <code>docker</code>:</p>\n<pre><code>administrator@ds1621plus:~$ sudo chown root:docker /var/run/docker.sock\nadministrator@ds1621plus:~$ ls -l /var/run/ | grep docker\ndrwx------ 8 root         root             200 Jun 10 17:40 docker\n-rw-r--r-- 1 root         root               5 Jun 10 17:40 docker.pid\nsrw-rw---- 1 root         docker             0 Jun 10 17:40 docker.sock\n</code></pre>\n<h4 id=\"test-non-root-user-access\" tabindex=\"-1\">Test non-root user access <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#test-non-root-user-access\" aria-hidden=\"true\">#</a></h4>\n<p>Now for the big moment. I logged out and back in again (for the new group membership to take effect) and tried a <code>docker</code> command:</p>\n<pre><code>administrator@ds1621plus:~$ logout\nConnection to ds1621plus closed.\n# ~\n; ssh ds1621plus\nadministrator@ds1621plus:~$ docker image ls\nREPOSITORY                     TAG       IMAGE ID       CREATED       SIZE\nhomeassistant/home-assistant   latest    832ca33fe14a   3 weeks ago   1.1GB\nlinuxserver/freshrss           latest    09ffc08f14fe   4 weeks ago   120MB\n</code></pre>\n<p>Success!</p>\n<h3 id=\"setting-up-ssh-access-and-a-docker-context\" tabindex=\"-1\">Setting up SSH access and a Docker context <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#setting-up-ssh-access-and-a-docker-context\" aria-hidden=\"true\">#</a></h3>\n<p>Now that I was able to safely interact with Docker on the NAS, I turned my attention to doing that remotely.</p>\n<p>Elsewhere in the Docker documentation, there's <a href=\"https://docs.docker.com/engine/security/protect-access/\">Protect the Docker daemon socket</a> which has tips on using either SSH or TLS to do so. I'd already established public key based SSH access from my local machine to the NAS, and maintain SSH configuration for various hosts (which you can see in my <a href=\"https://github.com/qmacro/dotfiles/blob/master/ssh/config\">dotfiles</a>). So the SSH route was appealing to me.</p>\n<p>The idea of this SSH access is to connect to the remote Docker service via <code>ssh</code> and run <code>docker</code> like that, remotely.</p>\n<p>However, trying a basic connection failed at first; running a simple <code>ssh</code>-based invocation of <code>docker</code> on the remote machine (<code>ssh ds1621plus docker -v</code>) resulted in an error that ended like this:</p>\n<p>&quot;<em>Exit status 127, please make sure the URL is valid, and Docker 18.09 or later is installed on the remote host: stderr=sh: docker: command not found</em>&quot;</p>\n<p>In desperation I even tried explicit values (<code>ssh -l administrator -p 2222 ds1621plus docker -v</code>) but got the same message.</p>\n<p>It turns out that on SSH access, the environment variables are not set the same as when you connect via <code>ssh</code> for an actual login session. Crucially, the value of the <code>PATH</code> environment variable was rather limited. Here's the entirety of the environment on an <code>ssh</code> based invocation of <code>env</code>:</p>\n<pre><code>; ssh ds1621plus env\nSHELL=/bin/sh\nSSH_CLIENT=192.168.86.50 54644 2222\nUSER=administrator\nMAIL=/var/mail/administrator\nPATH=/usr/bin:/bin:/usr/sbin:/sbin\nPWD=/volume1/homes/administrator\nSHLVL=1\nHOME=/var/services/homes/administrator\nLOGNAME=administrator\nSSH_CONNECTION=192.168.86.50 54644 192.168.86.155 2222\n_=/usr/bin/env\n</code></pre>\n<p>We can see that there are only four directories in the <code>PATH</code>: <code>/usr/bin</code>, <code>/bin</code>, <code>/usr/sbin</code> and <code>/sbin</code>.</p>\n<p>On the NAS, the <code>docker</code> client executable was in <code>/usr/local/bin</code>, not in the <code>PATH</code>; this was the cause of the error above - via a simple <code>ssh</code> invocation, the <code>docker</code> command wasn't found.</p>\n<p>So I had to address this, and I did via SSH's &quot;user environment&quot; feature.</p>\n<h4 id=\"turn-on-user-environment-support-in-sshd\" tabindex=\"-1\">Turn on user environment support in sshd <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#turn-on-user-environment-support-in-sshd\" aria-hidden=\"true\">#</a></h4>\n<p>SSH and its implementation, on client and server, is extremely accomplished, which is code for &quot;there's so much about SSH I don't yet know&quot;. One thing I learned about in this mini adventure is that the SSH daemon has support for &quot;user environments&quot;, via the <code>.ssh/environment</code> file, which is described in the <a href=\"http://man.openbsd.org/sshd.8#FILES\">FILES section of the sshd documentation</a>.</p>\n<p>Basically, setting the <code>PATH</code> to include <code>/usr/local/bin</code>, via this support for user environments, was exactly what I needed. What's more, I was not having to &quot;hack&quot; anything on the NAS (such as copying or symbolic-linking <code>docker</code> to another place so that it would be accessible) that I might regret later.</p>\n<p>First, though, I needed to turn on user environment support on the SSH daemon service on the NAS. For this, I uncommented <code>PermitUserEnvironment</code> in <code>/etc/ssh/sshd_config</code> and set the value to <code>yes</code>, with this result:</p>\n<pre><code>administrator@ds1621plus:~$ sudo grep PermitUserEnvironment /etc/ssh/sshd_config\nPermitUserEnvironment yes\n</code></pre>\n<p>I then restarted the NAS; I could have messed around finding a neater way just to restart the SSH daemon, but I'd read about some other gotchas doing this, and I was the only one using the NAS at the time, so I went for it.</p>\n<h4 id=\"add-the-location-of-the-docker-executable-in-the-path-variable-via-.sshenvironment\" tabindex=\"-1\">Add the location of the docker executable in the PATH variable via .ssh/environment <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#add-the-location-of-the-docker-executable-in-the-path-variable-via-.sshenvironment\" aria-hidden=\"true\">#</a></h4>\n<p>Now I could use the <code>.ssh/environment</code> file in the <code>administrator</code> user's home directory to set the value of the <code>PATH</code> environment variable to what I needed.</p>\n<p>To do this, I just started a remote login session on the NAS via <code>ssh</code>, and asked <code>env</code> to tell me what this was and also write it to the <code>.ssh/environment</code> file directly:</p>\n<pre><code>; ssh ds1621plus\nadministrator@ds1621plus:~$ env | grep PATH | tee .ssh/environment\nPATH=/sbin:/bin:/usr/sbin:/usr/bin:/usr/syno/sbin:/usr/syno/bin:/usr/local/sbin:/usr/local/bin\nadministrator@ds1621plus:~$\n</code></pre>\n<p>And that was it; when running commands remotely via <code>ssh</code>, this <code>PATH</code> value was now applicable. So the remote invocation of <code>docker</code> now worked:</p>\n<pre><code>; ssh ds1621plus docker -v\nDocker version 20.10.3, build b35e731\n</code></pre>\n<h4 id=\"create-a-docker-context\" tabindex=\"-1\">Create a Docker context <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#create-a-docker-context\" aria-hidden=\"true\">#</a></h4>\n<p>This final step was just for convenience, but worth it. With a context, I can avoid having to use <code>ssh</code> explicitly to interact with Docker on the NAS remotely.</p>\n<p>It's described in <a href=\"https://docs.docker.com/engine/security/protect-access/\">Use SSH to protect the Docker daemon socket</a> mentioned earlier, so I'll just show here what I did.</p>\n<p>Create the context:</p>\n<pre><code>; docker context create \\\n  --docker host=ssh://administrator@ds1621plus \\\n  --description=&quot;Synology NAS&quot; \\\n  synology\n</code></pre>\n<p>List the contexts, and select the new <code>synology</code> context for use:</p>\n<pre><code>; docker context list\nNAME                TYPE                DESCRIPTION                               DOCKER ENDPOINT                  KUBERNETES ENDPOINT                                                     ORCHESTRATOR\ndefault *           moby                Current DOCKER_HOST based configuration   unix:///var/run/docker.sock      https://api.c-681fdc3.kyma.shoot.live.k8s-hana.ondemand.com (default)   swarm\nsynology            moby                Synology NAS                              ssh://administrator@ds1621plus\n# ~\n; docker context use synology\nsynology\n# ~\n; docker image ls\nREPOSITORY                     TAG       IMAGE ID       CREATED       SIZE\nhomeassistant/home-assistant   latest    832ca33fe14a   4 weeks ago   1.1GB\nlinuxserver/freshrss           latest    09ffc08f14fe   4 weeks ago   120MB\n</code></pre>\n<p>Note that last command <code>docker image ls</code>; I invoked that on my client machine, but because of the context set, and the SSH based connection set up, the target was the Docker engine running on the Synology NAS. Success!</p>\n<h2 id=\"references\" tabindex=\"-1\">References <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-06-12-remote-access-to-docker-on-my-synology-nas/#references\" aria-hidden=\"true\">#</a></h2>\n<p>Here's what I read to find my way through this. Documents referenced in this post are also included here.</p>\n<ul>\n<li><a href=\"https://docs.docker.com/engine/security/protect-access/\">Protect the Docker daemon socket</a></li>\n<li><a href=\"https://github.com/docker/machine/issues/1200\">Feature request: add driver for Synology</a></li>\n<li><a href=\"https://github.com/microsoft/vscode-remote-release/issues/3748\">Remote Docker version not detected correctly</a></li>\n<li><a href=\"https://www.synoforum.com/threads/permissions-for-user-to-run-docker.3536/\">Permission for User to run Docker?</a></li>\n<li><a href=\"https://global.download.synology.com/download/Document/Software/DeveloperGuide/Firmware/DSM/All/enu/Synology_DiskStation_Administration_CLI_Guide.pdf\">CLI Administrator Guide for Synology NAS</a></li>\n<li><a href=\"http://man.openbsd.org/sshd.8\">OpenBSD manual page server for sshd</a></li>\n</ul>\n",
      "date_published": "2021-06-12T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-05-22-adding-a-drive-to-my-synology-nas/",
      "url": "https://qmacro.org/blog/posts/2021-05-22-adding-a-drive-to-my-synology-nas/",
      "title": "Adding a drive to my Synology NAS",
      "content_html": "<p><em>A brief summary of how things went adding a drive to my Synology DS1621+ NAS.</em></p>\n<p>Earlier this month I took delivery of my first Network Attached Storage (NAS) device - a <a href=\"https://www.synology.com/en-uk/products/DS1621+\">Synology DS1621+</a>. It has 6 drive bays. Note that you can sort of tell this from the model number:</p>\n<ul>\n<li>DS: Disk Station (standalone, as opposed to rack mountable, for example)</li>\n<li>16: Maximum number of drives possible</li>\n<li>21: Model year</li>\n</ul>\n<p>Synology also offer an expansion unit, a <a href=\"https://www.synology.com/en-uk/products/DX517\">DX517</a> which has 5 drive bays, and you can attach two of them to the DS1621+ adding up to a total of 6 + (5 + 5) = 16.</p>\n<blockquote>\n<p>Thanks to <a href=\"https://twitter.com/koehntopp\">Frank</a> for answering all my early questions on Synology NAS systems.</p>\n</blockquote>\n<h2 id=\"initial-setup\" tabindex=\"-1\">Initial setup <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-22-adding-a-drive-to-my-synology-nas/#initial-setup\" aria-hidden=\"true\">#</a></h2>\n<p>I bought two Seagate IronWolf 4TB drives. I knew I wanted to go for the <a href=\"https://www.synology.com/en-global/knowledgebase/DSM/tutorial/Storage/What_is_Synology_Hybrid_RAID_SHR\">Synology Hybrid Raid</a> (SHR) disk arrangement (this has many advantages that appealed to me, not least the ability to add different sized drives in the future).</p>\n<p>SHR requires at least two drives, which is why two was the minimum purchase that made sense. But I also bought a couple more, a week later. It's amazing how cheap, relatively speaking, spinning disk storage has become.</p>\n<p>With the initial two drives, I'd set up a storage pool, following the instructions (it was pretty straightforward). Here's what the status of that storage pool looked like:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/storage-manager-storage-pool.png\" alt=\"Status of Storage Pool 1 with 2 drives\" /></p>\n<p>There are a few things that are worth noting here:</p>\n<ul>\n<li>SHR is using &quot;1-drive fault tolerance&quot; which basically means that the capacity of one entire drive is given over to data safety and not available for actual storage</li>\n<li>this is why the &quot;Total capacity&quot; is at 3.63TB - remember that disk sizes are a bit misleading, this is effectively what you get with a 4TB drive, minus 0.01TB for overhead</li>\n<li>the &quot;Used capacity&quot; is at 250.01GB, as I've created a single volume of 250GB so far in that storage pool</li>\n</ul>\n<h2 id=\"inserting-the-new-drive\" tabindex=\"-1\">Inserting the new drive <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-22-adding-a-drive-to-my-synology-nas/#inserting-the-new-drive\" aria-hidden=\"true\">#</a></h2>\n<p>Now the second two drives have arrived, I decided to add one of them to the existing storage pool. I'm thinking of using the second one as a &quot;Hot Spare&quot; and seeing how that goes, but that's for another time.</p>\n<p>So I added it to the caddy:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/drive-in-caddy.jpg\" alt=\"drive in caddy\" /></p>\n<p>I checked the specifications of the DS1621+ and noted that it supported hot swapping, so I could insert the drive and caddy back in as the device was running, which I did:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/inserting-drive-into-nas.jpg\" alt=\"inserting drive into NAS\" /></p>\n<p>A few moments later, I re-checked the storage manager and it showed me the new drive:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/storage-manager-overview.png\" alt=\"storage manager overview\" /></p>\n<p>Here's the newly inserted drive in a &quot;Not initialized&quot; state in the HDD/SDD list:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/storage-manager-hdd-sdd.png\" alt=\"storage manager HDD/SDD list\" /></p>\n<h2 id=\"adding-the-new-drive-to-the-storage-pool\" tabindex=\"-1\">Adding the new drive to the storage pool <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-22-adding-a-drive-to-my-synology-nas/#adding-the-new-drive-to-the-storage-pool\" aria-hidden=\"true\">#</a></h2>\n<p>Now the drive was known, I could add it to the storage pool. I did this with the &quot;Action -&gt; Add drive&quot; menu item in the storage pool window, and the flow was fairly predictable, starting with the drive selection:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/storage-manager-storage-pool-add-drive-a.png\" alt=\"adding the new drive - choosing the drive\" /></p>\n<p>After a warning about any data being erased on the new drive, I was presented with a summary before proceeding:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/storage-manager-storage-pool-add-drive-b.png\" alt=\"adding the new drive - summary\" /></p>\n<p>The result was as expected. The storage pool had this new drive listed, and went into an &quot;Expanding&quot; status (note the capacity is not yet shown as being increased):</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/storage-manager-storage-pool-after-add.png\" alt=\"the third drive shown in the storage pool, which was now being expanded\" /></p>\n<p>Checking back over in the &quot;HDD/SDD&quot; display, the drive status has gone from &quot;Not initialized&quot; to &quot;Healthy&quot;, and is showing an assignment to Storage Pool 1.</p>\n<p>That was pretty much it - it wasn't an unexpected flow, but I was curious as to what would happen and how it would happen. Perhaps this helps someone who is also wondering. I've been writing this as I've been working through the flow, and now I've come to the end of this post, the status is still &quot;Expanding&quot; and shows that it's still less than 1% through a check of parity consistency - so it has a long way to go yet. I think I now realise why a &quot;Hot Spare&quot; might be useful. Anyway, I'll bring this post to an end now, and update it when the status changes.</p>\n<p>Update: 12 hours later, it's still at it - the status of the storage pool is &quot;Expanding (Checking parity consistency 47.33%)&quot;. Some way to go yet.</p>\n<p>Further update: It's the next morning, and the storage pool is now showing &quot;Healthy&quot; again, and its new expanded state of 7.27TB:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/storage-manager-storage-pool-expanded.png\" alt=\"storage pool in new expanded state\" /></p>\n<hr />\n<p>Further reading:</p>\n<ul>\n<li><a href=\"https://global.download.synology.com/download/Document/Hardware/HIG/DiskStation/21-year/DS1621+/enu/Syno_HIG_DS1621_Plus_enu.pdf\">Synology NAS DS1621+ Hardware Installation Guide</a> (PDF)</li>\n</ul>\n",
      "date_published": "2021-05-22T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-05-20-equality-in-tech/",
      "url": "https://qmacro.org/blog/posts/2021-05-20-equality-in-tech/",
      "title": "Equality in tech",
      "content_html": "<p><em>I support equality in tech, and so should you.</em></p>\n<p>This should go without saying, but alas, we're not in an ideal world. Equality in tech should be the backbone, the basis, upon which we run our industry. But it's not.</p>\n<p>I interact a lot on social media, I live stream too. And I haven't had a single occasion where I've been harrassed in any way. I'd like to think that this is because everyone is spellbound by what I have to say and what I'm showing. But it's not. It's because I'm male.</p>\n<p>I've had the pleasure of watching some awesome folks streaming on Twitch and YouTube, and have witnessed them being harrassed. And guess what, all of the targets, on all of the occasions, are female. This is not a coincidence.</p>\n<p>To those people thinking it's OK to make inappropriate comments, or worse, I say this: What is WRONG with you morons? It's not OK. Very not OK.</p>\n<p>To those who already get it, great. Perhaps the next step is to think about loading time in favour of helping and encouraging girls and women in tech. I've been very lucky to have been able to do this in a teaching capacity over the years, especially with youngsters (see reading links below). But even simpler is to just help female tech folks level up by supporting them on social media, helping them to grow and be the role models for the next generation too. And also, I've suddenly grokked it that being vocal about this also helps.</p>\n<p>I support equality in tech, and so should you.</p>\n<hr />\n<p>Further reading:</p>\n<ul>\n<li><a href=\"https://unbreak.tech/\">Unbreak Tech</a></li>\n<li><a href=\"https://qmacro.org/2012/12/05/codeclub-and-becoming-a-stem-ambassador/\">CodeClub and becoming a STEM Ambassador</a></li>\n<li><a href=\"https://qmacro.org/2013/04/18/codeclub-thoughts/\">CodeClub Thoughts</a></li>\n<li><a href=\"https://qmacro.org/2020/03/24/code-at-home/\">Let's learn to &quot;Code at Home&quot;</a></li>\n</ul>\n<p><img src=\"https://qmacro.org/blog/img/2020/03/underwaterscratch.jpg\" alt=\"Me teaching Scratch at Manchester CoderDojo\" /></p>\n",
      "date_published": "2021-05-20T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-05-19-supporting-developers-with-sponsorship/",
      "url": "https://qmacro.org/blog/posts/2021-05-19-supporting-developers-with-sponsorship/",
      "title": "Supporting developers with sponsorship",
      "content_html": "<p><em>I've started to use the sponsors facility on GitHub to support developers. Here's my thinking.</em></p>\n<p>There are many folks that I observe giving to the community. This giving takes many forms, such as providing software in an open source manner, supporting that software, sharing knowledge, and mentoring. I wanted to look into how I could provide a bit of support. I give to charity as part of my remuneration scheme, and I'm very fortunate to be able to do that. But that seems more of a &quot;given&quot; and not particularly specific, nor do I have any direct connection to the recipients.</p>\n<p>There are various ways to support individuals online - I've used the &quot;buy me a coffee&quot; approach, I've sent small amounts via PayPal to folks to say thanks (e.g. for the <a href=\"https://rubjo.github.io/victor-mono/\">Victor Mono font</a>), subscribed to folks on Twitch, gifted subscriptions, and so on. These are all avenues available to us, and I'd encourage you to look into them.</p>\n<p>But there's an avenue that resonates quite well with me, one that was introduced to me by <a href=\"https://github.com/alexellis\">Alex Ellis</a>. And that's <a href=\"https://github.com/sponsors\">GitHub Sponsors</a>. Subjective, I know, but I feel that sponsoring someone at this layer is a useful thing to do. The facilities offered by this mechanism also allow the sponsor relationship to be on a automatic and regular basis too.</p>\n<p>I've no idea how far I'll go yet, I'm just really starting. So far I'm sponsoring Alex for his work on Kubernetes, small machines and everything in between, and have also sponsored <a href=\"https://github.com/koalaman\">Vidar Holen</a>, mostly for <a href=\"https://github.com/koalaman/shellcheck\">shellcheck</a>, which has been a key part of how I'm <a href=\"https://qmacro.org/2020/10/05/improving-my-shell-scripting\">trying to improve my shell scripting</a>. I've just started sponsoring <a href=\"https://github.com/rwxrob\">Rob Muhlestein</a> for everything that he shares and for his long term efforts to share knowledge with junior developers on his Twitch live streams.</p>\n<p>My contributions are minimal, but this is a scale thing - I would like to encourage you to consider doing the same and sponsoring someone for their work that collectively helps strengthen our community.</p>\n<hr />\n<ul>\n<li>Update 25 May 2021 - I've now also started sponsoring <a href=\"https://github.com/sponsors/larshp\">Lars Hvam</a> for his work on ABAP outside the stack.</li>\n<li>Update 31 May 2021 - See also the <a href=\"https://github.com/junegunn/junegunn/blob/main/BACKERS.md\">BACKERS file for fzf's creator June Gunn</a></li>\n</ul>\n",
      "date_published": "2021-05-19T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-05-14-notes-on-markdown-linting---part-2/",
      "url": "https://qmacro.org/blog/posts/2021-05-14-notes-on-markdown-linting---part-2/",
      "title": "Notes on Markdown linting - part 2",
      "content_html": "<p><em>More on Markdown linting, this time in the context of GitHub Actions.</em></p>\n<p>Yesterday I <a href=\"https://qmacro.org/2021/05/13/notes-on-markdown-linting-1/\">wrote up some initial notes on my foray into Markdown linting</a>. Today I continue my journey of learning and discovery by attempting to get the Markdown linting working in a GitHub Action workflow, so I can have the checks done on pull requests.</p>\n<p>Beyond creating the workflow definition itself, there are only a few parts to getting Markdown content linted in the context of a pull request:</p>\n<ul>\n<li>getting the content of the pull request, to be able to perform linting upon it</li>\n<li>setting up an association between any linting output messages with the lines of Markdown to which they relate</li>\n<li>installing the <code>markdownlint</code> tool and any custom rule packages</li>\n<li>performing the linting</li>\n</ul>\n<h2 id=\"creating-the-workflow-definition\" tabindex=\"-1\">Creating the workflow definition <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-14-notes-on-markdown-linting---part-2/#creating-the-workflow-definition\" aria-hidden=\"true\">#</a></h2>\n<p>Since being able to quickly look at previous examples of GitHub Actions workflow definitions using my <a href=\"https://qmacro.org/2021/04/24/github-actions-workflow-browser/\">workflow browser</a>, it was quite easy to create a simple workflow definition. Here's what the start looks like:</p>\n<pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Markdown checks<br /><br /><span class=\"token key atrule\">on</span><span class=\"token punctuation\">:</span><br />  <span class=\"token key atrule\">pull_request</span><span class=\"token punctuation\">:</span><br />    <span class=\"token key atrule\">branches</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>main<span class=\"token punctuation\">,</span> master<span class=\"token punctuation\">]</span><br /><br /><span class=\"token key atrule\">jobs</span><span class=\"token punctuation\">:</span><br />  <span class=\"token key atrule\">lint-markdown</span><span class=\"token punctuation\">:</span><br />    <span class=\"token key atrule\">runs-on</span><span class=\"token punctuation\">:</span> ubuntu<span class=\"token punctuation\">-</span><span class=\"token number\">20.04</span><br /><br />    <span class=\"token key atrule\">steps</span><span class=\"token punctuation\">:</span><br /><br />    <span class=\"token punctuation\">...</span></code></pre>\n<blockquote>\n<p>I've moved from specifying <code>ubuntu-latest</code> to <code>ubuntu-nn.nn</code> for a more stable (or perhaps &quot;predictable&quot;) runner experience.</p>\n</blockquote>\n<p>Nothing exciting in this workflow definition so far; I've included both <code>main</code> and <code>master</code> in the list of branches because I've been testing with an older repository that still has <code>master</code> as the default branch.</p>\n<h2 id=\"getting-the-pull-request-content\" tabindex=\"-1\">Getting the pull request content <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-14-notes-on-markdown-linting---part-2/#getting-the-pull-request-content\" aria-hidden=\"true\">#</a></h2>\n<p>To run <code>markdownlint</code> on the content of the pull request, we need that in the runner workspace, and the usual use of the standard <a href=\"https://github.com/actions/checkout\">actions/checkout</a> action does the job here:</p>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/checkout@v2</code></pre>\n<h2 id=\"setting-up-an-output-association\" tabindex=\"-1\">Setting up an output association <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-14-notes-on-markdown-linting---part-2/#setting-up-an-output-association\" aria-hidden=\"true\">#</a></h2>\n<p>While the whole process will work without this step, it provides an extra level of comfort for those involved in the pull request review.</p>\n<p>The linting is performed in the runner, and the output (from <code>markdownlint</code>) is available in the workflow execution detail:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/execution-detail-messages.png\" alt=\"workflow execution detail showing markdown lint output\" /></p>\n<p>However, there's a small disconnect between the place of change and discussion (the pull request) and this workflow output.</p>\n<p>There's a special, slightly mysterious feature that can help address this disconnection. This is the &quot;matcher&quot; feature, and is mysterious in that it's not particularly prominent in the main <a href=\"https://docs.github.com/en/actions\">GitHub Actions documentation</a> ... although it is explained in the Actions Toolkit documentation, specifically <a href=\"https://github.com/actions/toolkit/blob/master/docs/commands.md#problem-matchers\">in the ::Commands section</a>.</p>\n<p>The general idea is that matchers can be added to a workflow execution. Matchers take the form of configuration that uses a regular expression to pick out parts of output messages and work out which bits are what. In other words, work out which file, line number and column each message applies to, as well as the message code and text.</p>\n<p>This is what a matcher looks like, and it's the one I'm using to match the <code>markdownlint</code> output:</p>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"problemMatcher\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><br />    <span class=\"token punctuation\">{</span><br />      <span class=\"token property\">\"owner\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"markdownlint\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"pattern\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><br />        <span class=\"token punctuation\">{</span><br />          <span class=\"token property\">\"regexp\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"^([^:]*):(\\\\d+):?(\\\\d+)?\\\\s([\\\\w-\\\\/]*)\\\\s(.*)$\"</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"file\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"line\"</span><span class=\"token operator\">:</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"column\"</span><span class=\"token operator\">:</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"code\"</span><span class=\"token operator\">:</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"message\"</span><span class=\"token operator\">:</span> <span class=\"token number\">5</span><br />        <span class=\"token punctuation\">}</span><br />      <span class=\"token punctuation\">]</span><br />    <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">]</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<blockquote>\n<p>The regular expression actually appears slightly more complex than it is, because the backslashes that are used to introduce the metacharacters <code>\\d</code> (digit), <code>\\s</code> (whitespace) and <code>\\w</code> (alphanumeric) are escaped with backslashes in the JSON string value (so e.g. <code>\\d</code> becomes <code>\\\\d</code>). This is so they don't get interpreted as escape characters themselves.</p>\n</blockquote>\n<p>If we stare at the output earlier, we see this:</p>\n<pre class=\"language-text\"><code class=\"language-text\">docs/b.md:5 MD022/blanks-around-headings/blanks-around-headers Headings should be surrounded by blank lines [Expected: 1; Actual: 0; Below] [Context: \"### Something Else\"]<br />docs/b.md:6 MD032/blanks-around-lists Lists should be surrounded by blank lines [Context: \"- one\"]<br />docs/b.md:10:10 MD011/no-reversed-links Reversed link syntax [(reversed)[https://qmacro.org]]</code></pre>\n<p>Applying the regular expression, we can see that it will indeed pick out the values as desired. Taking the last message line as an example, we get:</p>\n<table>\n<thead>\n<tr>\n<th>Regular expression part</th>\n<th>Matched text</th>\n<th>Value for</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>^</code></td>\n<td>(start of line)</td>\n<td></td>\n</tr>\n<tr>\n<td><code>([^:]*)</code></td>\n<td><code>docs/b.md</code></td>\n<td><code>file</code></td>\n</tr>\n<tr>\n<td><code>:</code></td>\n<td><code>:</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>(\\d+)</code></td>\n<td><code>10</code></td>\n<td><code>line</code></td>\n</tr>\n<tr>\n<td><code>:?</code></td>\n<td><code>:</code></td>\n<td></td>\n</tr>\n<tr>\n<td><code>(\\d+)?</code></td>\n<td><code>10</code></td>\n<td><code>column</code></td>\n</tr>\n<tr>\n<td><code>\\s</code></td>\n<td>(a space)</td>\n<td></td>\n</tr>\n<tr>\n<td><code>([\\w-\\/]*)</code></td>\n<td><code>MD011/no-reversed-links</code></td>\n<td><code>code</code></td>\n</tr>\n<tr>\n<td><code>\\s</code></td>\n<td>(a space)</td>\n<td></td>\n</tr>\n<tr>\n<td><code>(.*)</code></td>\n<td><code>Reversed link syntax [...]</code></td>\n<td><code>message</code></td>\n</tr>\n<tr>\n<td><code>$</code></td>\n<td>(end of line)</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>In this table, the escaping backslashes have been removed, as they're only there to make the JSON string happy.</p>\n</blockquote>\n<p>The result of having a matcher like this is that as well as having the messages available in the workflow execution detail, we get the messages in context too, which is far more comfortable. They appear in the workflow execution summary, like this (see the &quot;Annotations&quot; section):</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/execution-summary-messages.png\" alt=\"workflow execution summary showing markdown lint output\" /></p>\n<p>Moreover, each message appears directly below the line to which it applies, like this:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/messages-in-pr.png\" alt=\"markdown linting messages next to the relevant lines\" /></p>\n<p>To get this to work, the matcher configuration needs to be added with the <code>add-matcher</code> directive, in a step, like this:</p>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"echo ::add-matcher::.qmacro/workflows/markdownlint/problem-matcher.json\"</span></code></pre>\n<p>There is actually a GitHub Action, <a href=\"https://github.com/xt0rted/markdownlint-problem-matcher\">xt0rted/markdownlint-problem-matcher</a> that does this for you, but I'm still in two minds as to whether to use a &quot;black box&quot; action or something direct for things like this. Only time will tell.</p>\n<h2 id=\"installing-the-tool-and-custom-rules-packages\" tabindex=\"-1\">Installing the tool and custom rules packages <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-14-notes-on-markdown-linting---part-2/#installing-the-tool-and-custom-rules-packages\" aria-hidden=\"true\">#</a></h2>\n<p>Next, it's time to install the actual <code>markdownlint</code> tool, along with the custom rule package I mentioned in <a href=\"https://qmacro.org/2021/05/13/notes-on-markdown-linting-1/\">part 1</a>. While I installed <code>markdownlint</code> on my macOS machine with <code>brew</code>, it seems fine here to install it with <code>npm</code>, along with the rule too:</p>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\"><br />        npm install \\<br />          --no-package-lock \\<br />          --no-save \\<br />          markdownlint-cli markdownlint-rule-titlecase</span></code></pre>\n<p>Using the <code>--no-package-lock</code> and <code>--no-save</code> options makes for a slightly cleaner environment, given what we're doing here (i.e. we are only really interested in NPM metadata for this current job's execution).</p>\n<h2 id=\"performing-the-linting\" tabindex=\"-1\">Performing the linting <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-14-notes-on-markdown-linting---part-2/#performing-the-linting\" aria-hidden=\"true\">#</a></h2>\n<p>Now everything is ready, we can run the linter. I am invoking the <code>markdownlint</code> tool, just installed with <code>npm</code>, using the <code>npx</code> package runner as it seems the cleanest way to do it:</p>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\"><br />        npx markdownlint \\<br />          --config .qmacro/workflows/markdownlint/config.yaml \\<br />          --rules markdownlint-rule-titlecase \\<br />          docs/</span></code></pre>\n<p>Without configuration, <code>markdownlint</code> will apply <a href=\"https://github.com/DavidAnson/markdownlint/blob/main/doc/Rules.md\">all the rules</a> by default. I don't want that to happen, so I've used the <code>--config</code> option to point to a rules file <code>.qmacro/workflows/markdownlint/config.yaml</code>. This is what's in that file:</p>\n<pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token comment\"># All rules are inactive by default.</span><br /><span class=\"token key atrule\">default</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span><br /><br /><span class=\"token comment\"># These specific rules are active.</span><br /><span class=\"token comment\"># See https://github.com/DavidAnson/markdownlint#rules--aliases for details.</span><br /><span class=\"token key atrule\">heading-increment</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span><br /><span class=\"token key atrule\">no-reversed-links</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span><br /><span class=\"token key atrule\">no-missing-space-atx</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span><br /><span class=\"token key atrule\">no-multiple-space-atx</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span><br /><span class=\"token key atrule\">blanks-around-headings</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span><br /><span class=\"token key atrule\">blanks-around-lists</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span><br /><span class=\"token key atrule\">no-alt-text</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span></code></pre>\n<p>In other words, with this configuration, only those rules in that second stanza will be applied. Plus of course the explicit NPM package based title-case rule I've specified with the <code>--rules</code> option.</p>\n<blockquote>\n<p>I've been <a href=\"https://github.community/t/best-practices-for-storing-organising-shell-scripts-for-workflow-steps/176822\">thinking about</a> where to store workflow related artifacts in a repository. I don't want to use <code>.github/workflows</code> for anything other than actual workflow definition files. So right now, I'm thinking along the lines of a hidden user/organisation based directory name -- <code>.qmacro</code> in this example -- to parallel <code>.github</code>.</p>\n</blockquote>\n<p>The final thing to note in this invocation is that I'm passing a specific directory to be linted: <code>docs/</code>. This means only content there will be linted. I will probably want some sort of <code>.markdownlintignore</code> file at some stage, but for now this will do.</p>\n<h2 id=\"wrapping-up\" tabindex=\"-1\">Wrapping up <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-14-notes-on-markdown-linting---part-2/#wrapping-up\" aria-hidden=\"true\">#</a></h2>\n<p>Here's the workflow definition in its entirety:</p>\n<pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Markdown checks<br /><br /><span class=\"token key atrule\">on</span><span class=\"token punctuation\">:</span><br />  <span class=\"token key atrule\">pull_request</span><span class=\"token punctuation\">:</span><br />    <span class=\"token key atrule\">branches</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>main<span class=\"token punctuation\">,</span> master<span class=\"token punctuation\">]</span><br /><br /><span class=\"token key atrule\">jobs</span><span class=\"token punctuation\">:</span><br />  <span class=\"token key atrule\">main</span><span class=\"token punctuation\">:</span><br />    <span class=\"token key atrule\">runs-on</span><span class=\"token punctuation\">:</span> ubuntu<span class=\"token punctuation\">-</span><span class=\"token number\">20.04</span><br />    <span class=\"token key atrule\">steps</span><span class=\"token punctuation\">:</span><br /><br />    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/checkout@v2<br /><br />    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"echo ::add-matcher::.qmacro/workflows/markdownlint/problem-matcher.json\"</span><br /><br />    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\"><br />        npm install \\<br />          --no-package-lock \\<br />          --no-save \\<br />          markdownlint-cli markdownlint-rule-titlecase</span><br /><br />    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\"><br />        npx markdownlint \\<br />          --config .qmacro/workflows/markdownlint/config.yaml \\<br />          --rules markdownlint-rule-titlecase \\<br />          docs/</span></code></pre>\n<p>Everything works nicely, and I'm happy with the local and remote linting process.</p>\n",
      "date_published": "2021-05-14T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-05-13-notes-on-markdown-linting-1/",
      "url": "https://qmacro.org/blog/posts/2021-05-13-notes-on-markdown-linting-1/",
      "title": "Notes on Markdown linting - part 1",
      "content_html": "<p><em>Here's what I found out when I started to look into linting Markdown content.</em></p>\n<p>Thanks to some great direction and enlightenment from my colleague <a href=\"https://github.com/shegox\">Tobias</a>, I found myself getting my brain around Markdown linting. Of course, not what it is, but what the current possibilities are and how they might apply to my situation. I thought I'd write some notes on what I found (mostly for my future self).</p>\n<p>(See also <a href=\"https://qmacro.org/2021/05/14/notes-on-markdown-linting-part-2/\">Notes on Markdown linting - part 2</a> where I learn how to get Markdown linting working in GitHub Actions).</p>\n<h2 id=\"which-linter\" tabindex=\"-1\">Which linter? <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-13-notes-on-markdown-linting-1/#which-linter\" aria-hidden=\"true\">#</a></h2>\n<p>The Node.js-based <a href=\"https://github.com/DavidAnson/markdownlint\">DavidAnson/markdownlint</a> is the linter of choice. I'll refer to it as <code>markdownlint</code> in this post.</p>\n<p>There's a related project <a href=\"https://github.com/markdownlint/markdownlint\">markdownlint/markdownlint</a> which seems to be another, very similar linter written in Ruby. I'll refer to this as <code>mdl</code> as that's what the executable is called.</p>\n<p>They both seem to share the same <a href=\"https://github.com/DavidAnson/markdownlint/blob/main/doc/Rules.md\">rule</a> <a href=\"https://github.com/markdownlint/markdownlint/blob/master/docs/RULES.md\">definitions</a> which is good; although <code>mdl</code> seems to have rules that have been deprecated in <code>markdownlint</code>.</p>\n<p>I went for <code>markdownlint</code> for a number of reasons:</p>\n<ul>\n<li>it had more watchers, stargazers and forks on GitHub</li>\n<li>I could install it on my macOS daily driver with <code>brew</code> (see later)</li>\n<li>installing <code>mdl</code> involved RubyGems which I've never got on with</li>\n<li>there was explicit information about using <code>markdownlint</code> in various editors (including in Vim)</li>\n<li>Tobias had wanted to use a specific custom linting rule, something that <code>markdownlint</code> supports</li>\n</ul>\n<h2 id=\"installing-markdownlint\" tabindex=\"-1\">Installing markdownlint <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-13-notes-on-markdown-linting-1/#installing-markdownlint\" aria-hidden=\"true\">#</a></h2>\n<p>Markdownlint can be installed via <code>npm install</code> or via <code>brew</code>. The <code>brew</code> option is actually via a connected project <a href=\"https://github.com/igorshubovych/markdownlint-cli\">igorshubovych/markdownlint-cli</a>. I ran the <code>brew install markdownlint-cli</code> command and was up and running pretty much immediately:</p>\n<pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># ~/Projects/gh/github.com/qmacro/qmacro.github.io (markdownlint-post *=)</span><br /><span class=\"token punctuation\">;</span> markdownlint<br />Usage: markdownlint <span class=\"token punctuation\">[</span>options<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span>files<span class=\"token operator\">|</span>directories<span class=\"token operator\">|</span>globs<span class=\"token operator\">></span><br /><br />MarkdownLint Command Line Interface<br /><br />Options:<br /> -V, --version                               output the version number<br /> -c, --config <span class=\"token punctuation\">[</span>configFile<span class=\"token punctuation\">]</span>                   configuration <span class=\"token function\">file</span> <span class=\"token punctuation\">(</span>JSON, JSONC, JS, or YAML<span class=\"token punctuation\">)</span><br /> -d, --dot                                   include files/folders with a dot <span class=\"token punctuation\">(</span>for example <span class=\"token variable\"><span class=\"token variable\">`</span>.github<span class=\"token variable\">`</span></span><span class=\"token punctuation\">)</span><br /> -f, --fix                                   fix basic errors <span class=\"token punctuation\">(</span>does not work with STDIN<span class=\"token punctuation\">)</span><br /> -i, --ignore <span class=\"token punctuation\">[</span>file<span class=\"token operator\">|</span>directory<span class=\"token operator\">|</span>glob<span class=\"token punctuation\">]</span>          file<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> to ignore/exclude <span class=\"token punctuation\">(</span>default: <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><br /> -o, --output <span class=\"token punctuation\">[</span>outputFile<span class=\"token punctuation\">]</span>                   <span class=\"token function\">write</span> issues to <span class=\"token function\">file</span> <span class=\"token punctuation\">(</span>no console<span class=\"token punctuation\">)</span><br /> -p, --ignore-path <span class=\"token punctuation\">[</span>file<span class=\"token punctuation\">]</span>                    path to <span class=\"token function\">file</span> with ignore pattern<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><br /> -r, --rules  <span class=\"token punctuation\">[</span>file<span class=\"token operator\">|</span>directory<span class=\"token operator\">|</span>glob<span class=\"token operator\">|</span>package<span class=\"token punctuation\">]</span>  custom rule files <span class=\"token punctuation\">(</span>default: <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><br /> -s, --stdin                                 <span class=\"token builtin class-name\">read</span> from STDIN <span class=\"token punctuation\">(</span>does not work with files<span class=\"token punctuation\">)</span><br /> -h, --help                                  display <span class=\"token builtin class-name\">help</span> <span class=\"token keyword\">for</span> <span class=\"token builtin class-name\">command</span><br /></code></pre>\n<p>From the options we can see that it works in the way we'd expect - point it at one or more files, optionally give it some configuration, and go.</p>\n<p>But we can also see that it allows the use of custom rules. The custom rule that Tobias wanted to use was one that checks for title case (and I still went ahead, despite the fact that I dislike title case intensely :-)). The custom rules can be supplied in different forms as we can see from what can be specified with the <code>--rules</code> option; this particular one was of the exotic variety, i.e. an NPM package: <a href=\"https://www.npmjs.com/package/markdownlint-rule-titlecase\">markdownlint-rule-titlecase</a>. In fact, there's a grouping of NPM packages that are custom rules for <code>markdownlint</code>, organised via the <a href=\"https://www.npmjs.com/search?q=keywords:markdownlint-rule\">markdownlint-rule keyword</a>.</p>\n<h2 id=\"using-markdownlint-with-vim\" tabindex=\"-1\">Using markdownlint with Vim <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-13-notes-on-markdown-linting-1/#using-markdownlint-with-vim\" aria-hidden=\"true\">#</a></h2>\n<p>As I mentioned earlier, there is a <a href=\"https://github.com/DavidAnson/markdownlint#related\">list of references</a> to mechanisms where you can use <code>markdownlint</code> from the comfort of your editor. This list pointed to <a href=\"https://github.com/fannheyward/coc-markdownlint\">fannheyward/coc-markdownlint</a> for Vim.</p>\n<p>I don't use <a href=\"https://github.com/neoclide/coc.nvim\">Conqueror of Completion (coc)</a> - but I do use the <a href=\"https://github.com/dense-analysis/ale\">Asynchronous Linting Engine (ALE)</a>, which has <a href=\"https://github.com/dense-analysis/ale/blob/master/ale_linters/markdown/markdownlint.vim\">built-in support</a> for <code>markdownlint</code>. Within 5 minutes and a <a href=\"https://github.com/qmacro/dotfiles/commit/1281d8f908d51e43d280619668ac1d32bc3811a9\">few tweaks to my ALE related Vim configuration</a> I was up and running. I have to tweak the rule configuration to my liking, as right now, even as I write this post, I'm being given grief by <code>markdownlint</code> about <a href=\"https://github.com/DavidAnson/markdownlint/blob/main/doc/Rules.md#md013\">overly long lines</a>.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/long-lines-warnings.png\" alt=\"long lines warnings\" /></p>\n<h2 id=\"configuring-markdownlint\" tabindex=\"-1\">Configuring markdownlint <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-13-notes-on-markdown-linting-1/#configuring-markdownlint\" aria-hidden=\"true\">#</a></h2>\n<p>Configuration for <code>markdownlint</code> can be supplied with the <code>--config</code> option, or by configuration files in the right place - either in the current directory or in one's home directory.</p>\n<p>I added the following to <code>~/.markdownlintrc</code>, and the grief about line length went away:</p>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"line-length\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">false</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<h2 id=\"trying-a-custom-rule\" tabindex=\"-1\">Trying a custom rule <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-05-13-notes-on-markdown-linting-1/#trying-a-custom-rule\" aria-hidden=\"true\">#</a></h2>\n<p>I then wanted to see if I could get the custom linting rule working, at least in a basic way. On the <a href=\"https://www.npmjs.com/package/markdownlint-rule-titlecase\">NPM page for markdownlint-rule-titlecase</a> it says:</p>\n<p><em>Once installed using npm install markdownlint-rule-titlecase, run markdownlint with --rules &quot;markdownlint-rule-titlecase&quot;.</em></p>\n<p>Sounds fair, although a little worrying for me as I'm not going to be working with Markdown content in the context of a Node.js project any time soon. However, it turns out that I can still install the package and use it, even in a non-Node.js project directory:</p>\n<pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># ~/Projects/gh/github.com/qmacro/qmacro.github.io (markdownlint-post *=)</span><br /><span class=\"token punctuation\">;</span> <span class=\"token function\">npm</span> i --no-package-lock markdownlint-rule-titlecase<br /><span class=\"token function\">npm</span> WARN saveError ENOENT: no such <span class=\"token function\">file</span> or directory, <span class=\"token function\">open</span> <span class=\"token string\">'/Users/dj/Projects/gh/github.com/qmacro/qmacro.github.io/package.json'</span><br /><span class=\"token function\">npm</span> WARN enoent ENOENT: no such <span class=\"token function\">file</span> or directory, <span class=\"token function\">open</span> <span class=\"token string\">'/Users/dj/Projects/gh/github.com/qmacro/qmacro.github.io/package.json'</span><br /><span class=\"token function\">npm</span> WARN qmacro.github.io No description<br /><span class=\"token function\">npm</span> WARN qmacro.github.io No repository field.<br /><span class=\"token function\">npm</span> WARN qmacro.github.io No README data<br /><span class=\"token function\">npm</span> WARN qmacro.github.io No license field.<br /><br />+ markdownlint-rule-titlecase@0.1.0<br />added <span class=\"token number\">4</span> packages from <span class=\"token number\">4</span> contributors and audited <span class=\"token number\">4</span> packages <span class=\"token keyword\">in</span> <span class=\"token number\">0</span>.838s<br />found <span class=\"token number\">0</span> vulnerabilities</code></pre>\n<p>The warnings are fair - there isn't a <code>package.json</code> file of course, why would there be?</p>\n<p>I do now have a smallish <code>node_modules/</code> directory, though - containing the custom rule package:</p>\n<pre><code># ~/Projects/gh/github.com/qmacro/qmacro.github.io (markdownlint-post *%=)\n; tree -d node_modules/\nnode_modules/\n├── markdownlint-rule-helpers\n├── markdownlint-rule-titlecase\n├── title-case\n│   ├── dist\n│   └── dist.es2015\n└── tslib\n    └── modules\n\n7 directories\n</code></pre>\n<p>Oh well, I guess I could delete it when I'm done. In the meantime, can I take this new custom rule for a spin?</p>\n<pre><code># ~/Projects/gh/github.com/qmacro/qmacro.github.io (markdownlint-post *%=)\n; markdownlint --rules markdownlint-rule-titlecase _posts/2021-05-13-notes-on-markdown-linting.markdown\n_posts/2021-05-13-notes-on-markdown-linting.markdown:11:1 titlecase-rule Titlecase rule [Title Case: 'Expected ## Which Linter?, found ## Which linter?']\n_posts/2021-05-13-notes-on-markdown-linting.markdown:27:1 titlecase-rule Titlecase rule [Title Case: 'Expected ## Installing Markdownlint, found ## Installing markdownlint']\n_posts/2021-05-13-notes-on-markdown-linting.markdown:55:1 titlecase-rule Titlecase rule [Title Case: 'Expected ## Using Markdownlint with Vim, found ## Using markdownlint with Vim']\n_posts/2021-05-13-notes-on-markdown-linting.markdown:63:1 titlecase-rule Titlecase rule [Title Case: 'Expected ## Configuring Markdownlint, found ## Configuring markdownlint']\n_posts/2021-05-13-notes-on-markdown-linting.markdown:75:1 titlecase-rule Titlecase rule [Title Case: 'Expected ## Trying a Custom Rule, found ## Trying a custom rule']\n</code></pre>\n<p>Yes! Works nicely. Although like I say, I'm not sure why anyone would <em>want</em> to use such a rule ... I may write one that complains if you <em>do</em> use title case. But I digress.</p>\n<p>I think I'd like to be able to run these custom rules in Vim too, but I'll leave that for another time. I'm satisfied at least at this stage to be able to lint my Markdown files at all. And the next thing is actually to be able to use <code>markdownlint</code> in a GitHub Actions workflow.</p>\n<p>Update: <a href=\"https://qmacro.org/2021/05/14/notes-on-markdown-linting-part-2/\">I've written that up in part 2</a>.</p>\n",
      "date_published": "2021-05-13T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-05-10-solving-the-unrendered-markdown-headings/",
      "url": "https://qmacro.org/blog/posts/2021-05-10-solving-the-unrendered-markdown-headings/",
      "title": "Solving mysterious unrendered markdown headings",
      "content_html": "<p><em>I finally spent some time getting to the bottom of why some headings in my markdown content weren't getting rendered properly.</em></p>\n<p>I've noticed over the years that occasionally the rendered version of my markdown content, in particular on GitHub (which is where most of my markdown content ends up), sometimes contains unrendered headings. Here's <a href=\"https://github.com/qmacro-org/test/blob/d6f348858dd5014d8b96060e4b8dd75999af431b/README.md\">an example</a>:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/unrendered-heading-github.png\" alt=\"Rendered markdown showing an unrendered heading - on GitHub\" /></p>\n<p>The second level 2 heading &quot;Another heading level 2&quot; remains unrendered, even though everything looks fine. Why? This has bugged me for a while, but not so much as to make me stop and work out why it was happening. When it happened, I'd just go into the markdown source, rewrite the heading line, and all was fine.</p>\n<p>Today I finally stopped to spend a bit of time to look into it. Turns out it's quite simple and obvious now I know what was causing it.</p>\n<p>The <a href=\"https://www.markdownguide.org/basic-syntax/\">basic syntax</a> for headings involves one or more hashes (depending on the heading level needed) followed by the heading text. There's a space that should separate the hashes and the heading text. Here's an example:</p>\n<pre class=\"language-markdown\"><code class=\"language-markdown\"><span class=\"token title important\"><span class=\"token punctuation\">##</span> Heading level 2</span></code></pre>\n<p>What's causing that heading above not to be rendered properly? Well, it's the space. To you and me there is indeed a space between <code>##</code> and <code>Another heading level 2</code>.</p>\n<p>But it's <a href=\"https://en.wikipedia.org/wiki/The_wrong_type_of_snow\">the wrong type of space</a>.</p>\n<p>Checking first that it's not something weird going on with the markdown renderer on GitHub, let's try a different rendering, in the terminal, with the excellent <a href=\"https://github.com/charmbracelet/glow\">glow</a> tool:</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/05/unrendered-heading-glow.png\" alt=\"Rendered markdown showing an unrendered heading - using glow\" /></p>\n<p>Same issue.</p>\n<p>So let's dig in a little deeper, and look at the source.</p>\n<p>First, let's look at the first level 2 heading, which has been rendered correctly:</p>\n<pre><code># ~/Projects/gh/github.com/qmacro-org/test (main=)\n; grep 'Heading level 2' README.md | od -t x1 -c\n0000000    23  23  20  48  65  61  64  69  6e  67  20  6c  65  76  65  6c\n           #   #       H   e   a   d   i   n   g       l   e   v   e   l\n0000020    20  32  0a\n               2  \\n\n0000023\n# ~/Projects/gh/github.com/qmacro-org/test (main=)\n;\n</code></pre>\n<p>Seems OK, and yes, there's the space, hex value <code>20</code>, following the two hashes (hex values <code>23</code>).</p>\n<p>Now let's look at the second level 2 heading, which has not been correctly rendered;</p>\n<pre><code># ~/Projects/gh/github.com/qmacro-org/test (main=)\n; grep 'Another heading level 2' README.md | od -t x1 -c\n0000000    23  23  c2  a0  41  6e  6f  74  68  65  72  20  68  65  61  64\n           #   # 302 240   A   n   o   t   h   e   r       h   e   a   d\n0000020    69  6e  67  20  6c  65  76  65  6c  20  32  0a\n           i   n   g       l   e   v   e   l       2  \\n\n0000034\n# ~/Projects/gh/github.com/qmacro-org/test (main=)\n;\n</code></pre>\n<p>What the heck is that following the two hex <code>23</code> hash characters?</p>\n<pre><code>0000000    23  23  c2  a0\n           #   # 302 240\n</code></pre>\n<p>Turns out it's a <a href=\"https://en.wikipedia.org/wiki/Non-breaking_space\">non-breaking space</a> character. And its UTF-8 <a href=\"https://en.wikipedia.org/wiki/Non-breaking_space#Encodings\">encoding</a>, which is what the markdown file has, is <code>c2 a0</code>.</p>\n<p>So this second level 2 heading cannot be rendered as such, as the markdown cannot be recognised. Makes sense!</p>\n<p>But where are these non-breaking space coming from? How do they get there?</p>\n<p>Well, my daily driver during the working week is a macOS device, where it's notoriously more difficult that it should be to type a <code>#</code> character. One has to use <code>Option-3</code> (or <code>Alt-3</code>) to get it. And it turns out that after holding down <code>Option</code> to hit <code>3</code> a couple of times to introduce the <code>##</code> for a level 2 heading, my thumb is sometimes still on the <code>Option</code> key when I hit <code>space</code>.</p>\n<p>And guess what - <code>Option-space</code> is how you type a non-breaking space on macOS!</p>\n<p>So basically it's <strong>me</strong> that's been causing this issue - by inadvertently inserting not a space but a non-breaking space after the <code>#</code> characters introducing markdown headings.</p>\n",
      "date_published": "2021-05-10T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-04-24-github-actions-workflow-browser/",
      "url": "https://qmacro.org/blog/posts/2021-04-24-github-actions-workflow-browser/",
      "title": "GitHub Actions workflow browser",
      "content_html": "<p><em>I wrote a simple workflow browser to help me refer to previous workflow definitions while I'm writing a new one, as I'm still learning the syntax and techniques.</em></p>\n<p>With a programming or definition language, especially one that's new and powerful, it takes me a while to become comfortable writing scripts or definitions from scratch. I have a small amount of auto completion in my editor, but I'm not a fan - I prefer to learn by looking things up and then typing them in, rather than have words automatically completed for me.</p>\n<p>The YAML based syntax for definining GitHub Actions workflows is powerful and there are different ways of achieving similar things. And it's new to me too (although defining jobs and steps isn't - in many ways it's just like writing Job Control Language (JCL) back in the mainframe era, but that's a story for another time).</p>\n<p>While the latest version of the GitHub command line client <code>gh</code> sports lovely new features for workflows and actions, it doesn't quite give me the quick cross-repository overview that I'm looking for. So I decided to combine three of my favourite terminal power tools to help me:</p>\n<ul>\n<li>Bash, for my command line interface and utility scripting, my shell of choice</li>\n<li><a href=\"https://github.com/cli/cli\"><code>gh</code></a>, which is already a very accomplished command line interface (CLI) to GitHub and a really comfortable way of using the API</li>\n<li><a href=\"https://github.com/junegunn/fzf\"><code>fzf</code></a>, which is a powerful fuzzy finder utility and provides just enough features for me to build simple terminal user interfaces (UIs) with</li>\n</ul>\n<p>I combined them to build a &quot;workflow browser&quot;. Here it is in action:</p>\n<script id=\"asciicast-409638\" src=\"https://asciinema.org/a/409638.js\" async=\"\"></script>\n<p>It consists of three parts:</p>\n<ul>\n<li>\n<p>a new environment variable GH_CACHETIME which I can set globally to be nice to the GitHub API servers (I'm not changing workflows that often so a generous cache time of 1 hour works for me)</p>\n</li>\n<li>\n<p>the main <code>workflowbrowser</code> script which finds workflow definitions across my content on GitHub and presents them in a list to search through</p>\n</li>\n<li>\n<p>a separate <code>showgithubcontent</code> script that displays the content of a resource in one of my GitHub repositories</p>\n</li>\n</ul>\n<p>The <code>showgithubcontent</code> script was initially a function inside of the <code>workflowbrowser</code> script but I separated it out, first because it felt better and second because there was something more I could do once I'd browsed the workflow definitions with <code>workflowbrowser</code> and selected one - more on that later.</p>\n<h2 id=\"the-workflowbrowser-script\" tabindex=\"-1\">The workflowbrowser script <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-04-24-github-actions-workflow-browser/#the-workflowbrowser-script\" aria-hidden=\"true\">#</a></h2>\n<p>Here's the script in its entirety, <a href=\"https://github.com/qmacro/dotfiles/blob/master/scripts/workflowbrowser\">as it stands right now</a>:</p>\n<pre><code>#!/usr/bin/env bash\n\n# Find and browse GitHub Actions workflow definitions.\n# In addition to regular shell tools (such as sed), this\n# script uses gh and fzf.\n\nworkflows() {\n\n  # Takes owner type (org or user) and owner name.\n  # Returns tab-separated list of owner/repo/workflowfile/path.\n\n  local ownertype=$1\n  local owner=$2\n\n  gh api \\\n    --method GET \\\n    --paginate \\\n    --cache &quot;${GH_CACHETIME:-1h}&quot; \\\n    --field &quot;q=$ownertype:$owner path:.github/workflows/&quot; \\\n    --jq '.items[] | [&quot;\\(.repository.full_name)/\\(.name)&quot;, .repository.owner.login, .repository.name, .path] | @tsv' \\\n    &quot;/search/code&quot;\n\n}\n\nmain() {\n\n  # Calls workflows for my org and user.\n\n  cat \\\n    &lt;(workflows org qmacro-org) \\\n    &lt;(workflows user qmacro) \\\n    | fzf \\\n      --with-nth=1 \\\n      --delimiter='\\t' \\\n      --preview='showgithubcontent {2} {3} {4} yaml always' \\\n      | cut -f 2,3,4\n\n}\n\nmain &quot;$@&quot;\n</code></pre>\n<p>There's just a <code>main</code> function and a <code>workflows</code> function.</p>\n<h3 id=\"the-main-function\" tabindex=\"-1\">The main function <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-04-24-github-actions-workflow-browser/#the-main-function\" aria-hidden=\"true\">#</a></h3>\n<p>The <code>main</code> function calls the <code>workflows</code> function a couple of times, because I have repositories under my own user <a href=\"https://github.com/qmacro\">qmacro</a> and also under a small experimental organisation <a href=\"https://github.com/qmacro-org\">qmacro-org</a> and I have workflows across both of these owner areas.</p>\n<p>In learning more about Bash I've found it's helpful to know the terms for various aspects, so I'm going to point out one here. I'm calling the <code>workflows</code> function twice, like this:</p>\n<pre><code>  cat \\\n    &lt;(workflows org qmacro-org) \\\n    &lt;(workflows user qmacro)\n</code></pre>\n<p>This to me was the simplest way of combining output from two calls into a single stream, using <code>cat</code>. The <code>&lt;(...)</code> is <a href=\"https://tldp.org/LDP/abs/html/process-sub.html\">process substitution</a>. This is very useful when you want to supply some data to a command, which is expecting that data to be in a file, but where you don't have a file, and instead want to generate the data on the fly, and have it provided as the output of some execution. Here I'm using process substitution to call the <code>workflows</code> function a couple of times, and have the output from those calls supplied to <code>cat</code>. As if I did this: <code>cat firstfile secondfile</code>.</p>\n<blockquote>\n<p>I did a 10 minute video on process substitution on Hands-on SAP Dev, in case you're interested: <a href=\"https://www.youtube.com/watch?v=JF4lGw4Itpk&amp;list=PL6RpkC85SLQAIntm7MkNk78ysDm3Ua8t0&amp;index=41\">Ep.39 - Looking at process substitution</a>.</p>\n</blockquote>\n<p>I'll dig into the <code>workflows</code> function shortly, but for now, we need to know what it outputs, to understand better what we do with that output, i.e. what we do downstream from <code>cat</code> in the pipeline.</p>\n<p>The output from <code>workflows</code> are records representing workflow definitions, in the form of lines with tab-separated fields, like this:</p>\n<pre><code>qmacro-org/test/dump.yml\tqmacro-org\ttest\t.github/workflows/dump.yml\nqmacro/showntell/main.yml\tqmacro\tshowntell\t.github/workflows/main.yml\nqmacro/qmacro/build.yml\tqmacro\tqmacro\t.github/workflows/build.yml\n</code></pre>\n<p>In order, the fields represent:</p>\n<ol>\n<li>a combination of repo owner, repo name &amp; workflow definition file name</li>\n<li>the repo owner</li>\n<li>the repo name</li>\n<li>the path in the repository to that workflow definition file</li>\n</ol>\n<p>The lines are piped into <code>fzf</code> which is used to present the workflow definitions and also a preview of their contents. This is done by using various options supplied to <code>fzf</code>.</p>\n<p>The first option deals with what to show in the basic list display that <code>fzf</code> first presents, and that is the contents of the first field above (the combination). This is done using the <code>--with-nth</code> option; we also tell <code>fzf</code> how the fields are delimited:</p>\n<ul>\n<li><code>--with-nth=1</code> - use field 1 in the list display</li>\n<li><code>--delimiter='\\t'</code> - fields are tab-delimited</li>\n</ul>\n<p>Then there's what to do from a preview perspective; when a particular entry in the list is selected, <code>fzf</code> can run a preview command to display something in a window:</p>\n<ul>\n<li><code>--preview='showgithubcontent {2} {3} {4} yaml always'</code></li>\n</ul>\n<p>Whatever is produced (via STDOUT) by the incantation supplied with the <code>--preview</code> option is shown in the preview window. Here, we call the <code>showgithubcontent</code> script, supplying that script with 5 arguments. The first three use <code>fzf</code>'s field reference syntax to pass the values of the second, third &amp; fourth field, i.e. the repo owner, the repo name and the workflow file path. The last two arguments control how <code>showgithubcontent</code> displays things (we'll come to that later).</p>\n<p>With <code>fzf</code>, if an item in the list is indeed selected, then the line passed into <code>fzf</code> that represents the line selected is output to STDOUT. This makes <code>fzf</code> a very powerful tool that plays well with other tools, following the Unix philosophy (if no selection is made, e.g. by aborting <code>fzf</code> with Ctrl-C, then nothing is emitted).</p>\n<p>The final part of the <code>main</code> function takes the line emitted from <code>fzf</code> and outputs the same three fields (repo owner, repo name and workflow file path). Basically field 1 is just used as a &quot;display&quot; field for <code>fzf</code>.</p>\n<h3 id=\"the-workflows-function\" tabindex=\"-1\">The workflows function <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-04-24-github-actions-workflow-browser/#the-workflows-function\" aria-hidden=\"true\">#</a></h3>\n<p>The <code>workflows</code> function is basically a wrapper around a call to the <a href=\"https://docs.github.com/en/rest/reference/search\">GitHub Search API</a>. This is an API that I haven't used before now, and it's pretty powerful. There are different endpoints representing different search approaches. What worked for me, to find workflow definitions, was to use the <a href=\"https://docs.github.com/en/rest/reference/search#search-code\">Search code</a> endpoint with <code>/search/code</code>.</p>\n<p>This endpoint takes the search criteria in the form of a query string parameter <code>q</code>, and it was very easy to use the GUI based search to try out different search parameters to figure out what I needed to specify. Here's an example:</p>\n<p><a href=\"https://github.com/search?q=org%3Aqmacro-org+path%3A.github%2Fworkflows%2F\">https://github.com/search?q=org%3Aqmacro-org+path%3A.github%2Fworkflows%2F</a></p>\n<p>One thing that tripped me up at first was the wrong type of request was being made first of all. I supplied the search criteria value in the <code>q</code> query string parameter correctly, like this (as you can see in the function):</p>\n<ul>\n<li><code>--field &quot;q=$ownertype:$owner path:.github/workflows/&quot;</code></li>\n</ul>\n<p>but the HTTP call that <code>gh</code> then made for me was a POST request, with this search query parameter in the body of the request. That wasn't right. Checking in the API documentation, the <code>q</code> parameter needs to be in the query string. Explicitly setting the method to GET made this right:</p>\n<ul>\n<li><code>--method GET</code></li>\n</ul>\n<p>There are a couple of other &quot;housekeeping&quot; parameters used here too:</p>\n<ul>\n<li><code>--paginate</code></li>\n<li><code>--cache &quot;${GH_CACHETIME:-1h}&quot;</code></li>\n</ul>\n<p>I don't yet have that many workflow definitions, but if it comes to that, <code>gh</code> will work through the responses to get them all for me with <code>--paginate</code>.<br />\nAnd the <code>--cache</code> parameter works both ways: my activities are well behaved when it comes to using the API endpoints, and also, after the first time the list of workflow definitions is retrieved, any subsequent uses of the workflow browser are that much snappier (this works also with the similar use of the <code>--cache</code> parameter in the <code>showgithubcontent</code> script we'll see shortly). Note that if there's no value specified for <code>GH_CACHETIME</code>, the default will be 1 hour (<code>1h</code>) through the use of <a href=\"https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html\">shell parameter expansion</a>.</p>\n<p>Next we come to the use of the <code>--field</code> parameter, which allows me to specify the name and value for the search parameter <code>q</code>. I looked at the <a href=\"https://docs.github.com/en/github/searching-for-information-on-github/searching-code\">Searching code</a> documentation to find out about the <code>ownertype:owner</code> specification. The first time around this value will be &quot;org:qmacro-org&quot; and the second time around it will be &quot;user:qmacro&quot;. Moreover, with <code>path</code> I can search for content that appears at a specific location - see <a href=\"https://docs.github.com/en/github/searching-for-information-on-github/searching-code#search-by-file-location\">Search by file location</a>.</p>\n<blockquote>\n<p>For those wondering, GitHub Actions workflow definition files are stored in the <code>.github/workflows/</code> directory within a repository.</p>\n</blockquote>\n<p>Last but not least I use <code>--jq</code> parameter to supply <code>fzf</code> with some <code>jq</code> script that will parse and extract the data I need from the API's JSON output. I think it was in <a href=\"https://github.com/cli/cli/releases/tag/v1.7.0\">release 1.7.0</a> that this feature appeared, and it's a great idea - build in <code>jq</code> to <code>gh</code> so those that don't have <code>jq</code> already installed can still benefit. I guess it also helps to establish <code>jq</code> as the de facto standard for parsing and manipulating JSON.</p>\n<p>If we add some whitespace to the <code>jq</code> script passed with the <code>--jq</code> parameter, we get this:</p>\n<pre class=\"language-jq\"><code class=\"language-jq\"><span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><br />  <span class=\"token operator pipe\">|</span> <span class=\"token punctuation\">[</span><br />      <span class=\"token string\">\"<span class=\"token interpolation\"><span class=\"token punctuation\">\\(</span><span class=\"token content\"><span class=\"token punctuation\">.</span>repository<span class=\"token punctuation\">.</span>full_name</span><span class=\"token punctuation\">)</span></span>/<span class=\"token interpolation\"><span class=\"token punctuation\">\\(</span><span class=\"token content\"><span class=\"token punctuation\">.</span>name</span><span class=\"token punctuation\">)</span></span>\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token punctuation\">.</span>repository<span class=\"token punctuation\">.</span>owner<span class=\"token punctuation\">.</span>login<span class=\"token punctuation\">,</span><br />      <span class=\"token punctuation\">.</span>repository<span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">,</span><br />      <span class=\"token punctuation\">.</span>path<br />    <span class=\"token punctuation\">]</span><br />  <span class=\"token operator pipe\">|</span> @tsv'</code></pre>\n<p>I think it's always easier to stare at a script like this when we see what it's going to be processing, so here's some sample output from the API call to the search endpoint (reduced for brevity):</p>\n<pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span><br />  <span class=\"token property\">\"total_count\"</span><span class=\"token operator\">:</span> <span class=\"token number\">7</span><span class=\"token punctuation\">,</span><br />  <span class=\"token property\">\"incomplete_results\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">false</span><span class=\"token punctuation\">,</span><br />  <span class=\"token property\">\"items\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><br />    <span class=\"token punctuation\">{</span><br />      <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"dump.yml\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"path\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\".github/workflows/dump.yml\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"repository\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />        <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">331995789</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"test\"</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"full_name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"qmacro-org/test\"</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"owner\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />          <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"qmacro-org\"</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">75827316</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Organization\"</span><br />        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />    <span class=\"token punctuation\">{</span><br />      <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"main.yml\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"path\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\".github/workflows/main.yml\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"repository\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />        <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">331995789</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"showntell\"</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"full_name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"qmacro/showntell\"</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"owner\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />          <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"qmacro\"</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">73068</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"User\"</span><br />        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />    <span class=\"token punctuation\">{</span><br />      <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"build.yml\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"path\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\".github/workflows/build.yml\"</span><span class=\"token punctuation\">,</span><br />      <span class=\"token property\">\"repository\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />        <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">165207450</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"qmacro\"</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"full_name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"qmacro/qmacro\"</span><span class=\"token punctuation\">,</span><br />        <span class=\"token property\">\"owner\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><br />          <span class=\"token property\">\"login\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"qmacro\"</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">73068</span><span class=\"token punctuation\">,</span><br />          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"User\"</span><br />        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><br />    <span class=\"token punctuation\">}</span><br />  <span class=\"token punctuation\">]</span><br /><span class=\"token punctuation\">}</span></code></pre>\n<p>Now we can understand what the <code>jq</code> script is doing. It's working through the contents of the <code>items</code> array (the search results) and piping each item into an array construction. The array construction is declaring three fields, a literal string and two properties:</p>\n<ul>\n<li>\n<p>the literal string contains two JSON properties <code>.repository.full_name</code> and <code>.name</code>, which are references with the <code>\\(...)</code> syntax. They're put into a literal string so I can add a slash (<code>/</code>) between them</p>\n</li>\n<li>\n<p>the two properties are the repository owner name and the repository name</p>\n</li>\n</ul>\n<p>Once constructed, the array is passed to <code>@tsv</code> which puts the values into a nice tab-separated list.</p>\n<blockquote>\n<p>I think it's fair to say that the output from the built-in <code>jq</code> works as if the <code>--raw-output</code> flag has been specified (see the <a href=\"https://stedolan.github.io/jq/manual/\">jq Manual</a>), which is what we want.</p>\n</blockquote>\n<p>This then produces the lines that we've seen earlier, i.e. ones that look like this:</p>\n<pre><code>qmacro-org/test/dump.yml\tqmacro-org\ttest\t.github/workflows/dump.yml\nqmacro/showntell/main.yml\tqmacro\tshowntell\t.github/workflows/main.yml\nqmacro/qmacro/build.yml\tqmacro\tqmacro\t.github/workflows/build.yml\n</code></pre>\n<p>These lines are then ready for piping to <code>fzf</code> in <code>main()</code>. Great!</p>\n<p>Now let's move on to the second script, which is what <code>fzf</code> calls to present the previews (i.e. with <code>--preview='showgithubcontent {2} {3} {4} yaml always'</code>).</p>\n<h2 id=\"the-showgithubcontent-script\" tabindex=\"-1\">The showgithubcontent script <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-04-24-github-actions-workflow-browser/#the-showgithubcontent-script\" aria-hidden=\"true\">#</a></h2>\n<p>As I mentioned earlier, this was originally just another function inside the <code>workflowbrowser</code> script, but I extracted it to use outside of that script too. You'll see why in a bit.</p>\n<p>Here's the script in its entirety:</p>\n<pre><code>#!/usr/bin/env bash\n\n# Takes owner, repo and path and shows content of that resource from GitHub.\n# Also accepts optional language and colour parameter.\n# Uses gh, base64 and bat.\n\ndeclare owner=$1\ndeclare repo=$2\ndeclare path=$3\ndeclare language=&quot;${4:-txt}&quot;\ndeclare color=&quot;${5:-never}&quot;\n\ngh api \\\n  --cache &quot;${GH_CACHETIME:-1h}&quot; \\\n  --jq '.content' \\\n  &quot;/repos/$owner/$repo/contents/$path&quot; \\\n  | base64 --decode -i - \\\n  | bat --color &quot;$color&quot; --theme gruvbox --plain --language &quot;$language&quot; -\n</code></pre>\n<p>This is so simple as to not even warrant the <code>main()</code> function based approach to organisation. At least not yet. So what does it do? It expects five parameters, that we've seen already:</p>\n<ul>\n<li>the repository owner name</li>\n<li>the repository name</li>\n<li>the path to the workflow definition file in the repository</li>\n<li>what language to base any syntax highlighting upon</li>\n<li>whether to show colours</li>\n</ul>\n<p>The last two parameters are specific to the <code>bat</code> tool, which is a posh version of <code>cat</code> - <a href=\"https://github.com/sharkdp/bat\"><code>bat</code>'s home page</a> calls it &quot;a cat clone with wings&quot;.</p>\n<p>The reason we need the first three parameters is because they're required in the call we need to the <a href=\"https://docs.github.com/en/rest/reference/repos#get-repository-content\">GitHub Contents API</a>. With this endpoint:</p>\n<pre><code>/repos/{owner}/{repo}/contents/{path}\n</code></pre>\n<p>we can retrieve the contents of a resource (a file) in a repository.</p>\n<p>Let's have a look what this gives us, in a sample call, for the following values:</p>\n<ul>\n<li>owner: &quot;qmacro&quot;</li>\n<li>repo: &quot;showntell&quot;</li>\n<li>path: &quot;.github/workflows/main.yml&quot;</li>\n</ul>\n<pre><code>$ gh api /repos/qmacro/showntell/contents/.github/workflows/main.yml\n\n{\n  &quot;name&quot;: &quot;main.yml&quot;,\n  &quot;path&quot;: &quot;.github/workflows/main.yml&quot;,\n  &quot;size&quot;: 387,\n  &quot;url&quot;: &quot;https://api.github.com/repos/qmacro/showntell/contents/.github/workflows/main.yml?ref=master&quot;,\n  &quot;type&quot;: &quot;file&quot;,\n  &quot;content&quot;: &quot;bmFtZTogYWRkX2FjdGl2aXR5X2NhcmQKCm9uOgogIGlzc3VlczoKICAgIHR5\\ncGVzOiBvcGVuZWQKCmpvYnM6CiAgbGlzdF9wcm9qZWN0czoKICAgIHJ1bnMt\\nb246IHVidW50dS1sYXRlc3QKICAgIG5hbWU6IEFzc2lnbiBuZXcgaXNzdWUg\\ndG8gcHJvamVjdAogICAgc3RlcHM6CiAgICAtIG5hbWU6IENyZWF0ZSBuZXcg\\ncHJvamVjdCBjYXJkIHdpdGggaXNzdWUKICAgICAgaWQ6IGxpc3QKICAgICAg\\ndXNlczogcW1hY3JvL2FjdGlvbi1hZGQtaXNzdWUtdG8tcHJvamVjdC1jb2x1\\nbW5AcmVsZWFzZXMvdjEKICAgICAgd2l0aDoKICAgICAgICB0b2tlbjogJHt7\\nIHNlY3JldHMuR0lUSFVCX1RPS0VOIH19CiAgICAgICAgcHJvamVjdDogJ3Ax\\nJwogICAgICAgIGNvbHVtbjogJ3RoaW5ncycK\\n&quot;,\n  &quot;encoding&quot;: &quot;base64&quot;\n}\n</code></pre>\n<p>(Output is reduced for brevity again).</p>\n<p>The content isn't what we might first expect - where's the YAML? It's Base64 encoded, so we need to grab the value of the <code>content</code> property (which we do with <code>--jq '.content'</code>) and decode it. The handy <code>base64</code> command is ideal for that.</p>\n<p>Once decoded, the workflow definition YAML content is piped into <code>bat</code>, with the following parameters:</p>\n<ul>\n<li><code>--color &quot;$color&quot;</code> - do we want colour? In preview mode, always (which is why we pass <code>always</code> in the call from the other script) but unless we're explicit about that, <code>bat</code> won't use colour. This is because of the shell parameter expansion in the declaration of the <code>color</code> variable: <code>&quot;${5:-never}&quot;</code>, where the literal string &quot;never&quot; is used as a default value if none is supplied.</li>\n<li><code>--theme gruvbox</code> - who doesn't like a little <a href=\"https://github.com/morhetz/gruvbox\">gruvbox</a> theming?</li>\n<li><code>--plain</code> - this turns off any of the <code>bat</code> &quot;chrome&quot; like line numbers and headings.</li>\n<li><code>--language &quot;$language&quot;</code> - this tells <code>bat</code> about the content, in the form of a hint as to what language it is and therefore how to syntax highlight it.</li>\n</ul>\n<p>And don't miss the final <code>-</code> passed to <code>bat</code>, that's to tell it to read from STDIN.</p>\n<p><strong>Embracing the Unix philosophy</strong></p>\n<p>That'a about it for the two scripts. I've found them to be useful and have had fun creating them. Really it's just glueing together different tools, that's sort of the point, part of the Unix philosophy in general.</p>\n<p>And talking of that, here's the reason I split out the <code>showgithubcontent</code> function into a separate script. It's because I wanted to be able to browse the workflow definitions, but then if I selected one, I wanted to be taken into an editor with that definition's contents. And with a proper shell (like Bash, or most other Unix shells) this is simple:</p>\n<pre><code>$ workflowbrowser | xargs showgithubcontent | vim --not-a-term -\n</code></pre>\n<p>That is:</p>\n<ol>\n<li>call <code>workflowbrowser</code> and take the selected output from <code>workflowbrowser</code> (which will be the three values that <code>fzf</code> emits when I select a workflow definition) and, by piping them through to a call to <code>xargs</code>, send them as parameters to <code>showgithubcontent</code></li>\n<li>this of course is a &quot;second&quot; call to <code>showgithubcontent</code> - it's been used in <code>fzf</code>'s preview window, but now we're calling it explicitly, for the selected definition, without the two extra arguments &quot;yaml&quot; and &quot;always&quot; so that the the workflow definition is output without adornment</li>\n<li>that unadorned workflow definition goes to STDOUT, which is then fed through the pipe to the STDIN of <code>vim</code>, my editor, where I tell it to read from STDIN (that's the use of <code>-</code>) and, using <code>--not-a-term</code>, tell it that its startup context is not a terminal (it's a pipe) so that it won't issue any warnings along those lines</li>\n</ol>\n<p>Here's an example of that pipeline flow in action:</p>\n<script id=\"asciicast-409639\" src=\"https://asciinema.org/a/409639.js\" async=\"\"></script>\n<p>I hope you found this useful and perhaps it will encourage you to create your own utility scripts using <code>gh</code> and <code>fzf</code>.</p>\n",
      "date_published": "2021-04-24T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-04-04-mainframes,-sdsf-and-github-actions/",
      "url": "https://qmacro.org/blog/posts/2021-04-04-mainframes,-sdsf-and-github-actions/",
      "title": "Mainframes, SDSF and GitHub Actions",
      "content_html": "<p><em>GitHub Actions workflows, mainframes and SDSF. I can't get the combination out of my head.</em></p>\n<p>I started my computing adventure at the age of 11 on a minicomputer (see <a href=\"https://qmacro.org/2020/11/03/computer-unit-1979/\">Computer Unit 1979</a>) and then IBM mainframes featured heavily in the early and formational part of my career. Job definitions, job &amp; step interdependencies, batch job execution and output management are in my blood.</p>\n<p>The reliability, predictability, and perhaps even the ceremony of defining a job, submitting it to the right execution class, have it run, at some time, and then poring over the output after execution was finished, is something that still appeals to me. Even in today's world of always-on, I'd like to think that realtime, the ultimate opposite to batch, is in some senses overrated, or at least misunderstood.</p>\n<p>All of my career, more or less, has revolved around SAP systems. The R in SAP R/2, which I worked with between 1987 and around 1995, stood for Realtime (and this was the name of the consulting company I joined to launch my career as a consultant / contractor, but that's a story for another time).</p>\n<p><strong>Realtime vs batch</strong></p>\n<p>What did realtime mean here? Well, it meant that human facing processes were exposed via screens, interaction with data was live, it happened there and then. Transactions could be executed directly. What SAP R/2 replaced was a completely batch oriented system where everything ran asynchronously and the idea of screens allowing access to and interaction with business processes was very new. Moreover, these business processes were integrated.</p>\n<p>Of course, any SAP Basis person will tell you that while yes there are dynamic programs (dynpros) that allow immediate and interactive access in realtime, the batch concept is still alive and well in SAP systems. It was then in R/2 (with an overnight schedule of tens if not hundreds of interdependent jobs), and it even is today with SAP S/4HANA Cloud, and every other SAP system that is based upon the R/3 architecture. Yes, I'm talking about the batch processes, and even the update processes, that are part of the DISP+WORK design from the early 1990s.</p>\n<p>So batch is still alive and well, in fact it never went away.</p>\n<p>Moreover, while for very large organisations the mainframe lives on, especially in financial circles, the concept of the mainframe lives on too. <a href=\"http://www.winestockwebdesign.com/Essays/Eternal_Mainframe.html\">The Eternal Mainframe</a> is a great essay that muses on that and more.</p>\n<p><strong>Realtime vs resilient</strong></p>\n<p>And in today's era, the obsession with realtime seems to be spilling over into the API world, where folks are wanting to interconnect their systems in a loosely coupled way with realtime interfaces. While <a href=\"https://en.wikipedia.org/wiki/Loose_coupling\">loose coupling</a> is usually the right approach, realtime interfaces are a different beast. In some cases of course, synchronous communication, with blocking, is required. But in many cases it's not.</p>\n<p><em>What the R should really stand for here is not Realtime, but Resilient.</em></p>\n<p>(I'd like to take credit for this quotable nugget, but I have to attribute it to the person from whom I heard it first - my friend and SAP colleague <a href=\"https://people.sap.com/c.stasila\">Craig Stasila</a>.)</p>\n<p>And what does that mean, exactly? Well to me it means not synchronous, but asynchronous. Message (i.e. event) based integration. Message events that are fired by a system, with a payload, managed by a message bus, and received &amp; processed by other systems. We've looked into this a lot on our <a href=\"https://blogs.sap.com/2020/11/09/an-overview-of-sap-developers-video-content/#shows\">Hands-on SAP Dev</a> show, in particular the <a href=\"https://www.youtube.com/playlist?list=PL6RpkC85SLQCf--P9o7DtfjEcucimapUf\">Diving into SAP Enterprise Messaging</a> series (SAP's Enterprise Messaging service is now called Event Mesh, by the way).</p>\n<p>Embracing &amp; understanding the importance of this asynchronous nature might help folks to think about the nature of batch, too. Not everything needs to be immediate. Not everything must happen as soon as something else happens. If that was the case, then why are we seeing such a massive interest and use of <a href=\"https://github.com/features/actions\">GitHub Actions</a>, which brings the whole idea, and appeal, of batch processing to the masses.</p>\n<p><strong>GitHub Actions and batch processing</strong></p>\n<p>While writing this I've realised that there's another layer to GitHub Actions that adds to the appeal for me. When I first encountered batch processing, at Esso Petroleum at the start of my career, I spent many a happy hour writing Job Control Language (JCL), monitoring jobs, and obsessing over the detail of their output messages. One thing that was almost unspoken in this is that sitting at my silent terminal, I had no idea at the time where the machines were that processed my jobs, what they looked like, sounded like, nor did I have to care. They were looked after by the system operators.</p>\n<p>And so it is with GitHub Actions. Unless I'm using self-hosted runners, I have no idea about the machines upon which the jobs defined in my workflows are run. I don't know where they are, whether they're real or virtual, nothing. And as long as I remain within my execution quota, I don't have to care, either. Again, that's someone else's task.</p>\n<p><strong>SDSF for GitHub Actions</strong></p>\n<p>Anyway, I'm not really sure where I'm going with this post. I'd started out with the intention of explaining a little bit as to why, to GitHub Actions product manager <a href=\"https://twitter.com/chrisrpatterson\">Chris Patterson</a>'s question &quot;If you had one wish for GitHub Actions what would it be?&quot;, my answer was:</p>\n<p><a href=\"https://twitter.com/qmacro/status/1372578449743826949\">&quot;SDSF for workflow/job execution and output. Please :-)&quot;</a></p>\n<p>IBM's System Display and Search Facility (<a href=\"https://en.wikipedia.org/wiki/SDSF\">SDSF</a>) was how I navigated the output from batch jobs that had executed. How I searched, sorted, viewed, printed and purged output. How I found patterns in what was happening in the area for which I was responsible. Using a powerful and classic terminal user interface (TUI) design which fit well with the Interactive System Productivity Facility (ISPF) world where we spent our working hours.</p>\n<p>I think I'll leave the explanation for why I think it would suit the GitHub Actions ecosystem, for next time. Until then, I'll leave you with a screenshot (courtesy of <a href=\"https://community.ibm.com/community/user/ibmz-and-linuxone/blogs/trent-balta1/2021/01/28/exploring-zos-through-zowe-zoau-ide-tools\">Trent Balta and the IBM Community</a>) of SDSF in action.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/04/sdsf.png\" alt=\"SDSF in action\" /></p>\n",
      "date_published": "2021-04-04T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-03-31-deeper-connections-to-everyday-tools/",
      "url": "https://qmacro.org/blog/posts/2021-03-31-deeper-connections-to-everyday-tools/",
      "title": "Deeper connections to everyday tools",
      "content_html": "<p><em>With some things, ignorance is not bliss. There are entire features of the tools I use every day that I know little about, and I want to change that.</em></p>\n<p>Something mildly profound emerged from the combination two recent activities:</p>\n<ul>\n<li>eBay sales of items I've not used in a while</li>\n<li>maintenance of my La Pavoni coffee machine</li>\n</ul>\n<p>The <a href=\"https://qmacro.org/2021/03/27/la-pavoni-maintenance-successful/\">successful maintenance</a> of that beautifully designed manual lever espresso machine did take a while, but during it I guess I formed a deeper relationship with the device, built upon the existing connection I had already from the constant enjoyment &amp; challenge of getting everything aligned to pull a decent shot.</p>\n<p>And the items I sold (SONOS speakers, an old Macbook Pro) are items I've not really had any relationship with at all. Yes, I used the speakers, but not every day, and since SONOS's <a href=\"https://www.fastcompany.com/90454672/this-is-disgusting-angry-sonos-customers-are-calling-for-a-boycott\">meltdown</a> last year an active distancing and dislike has grown between me and the devices.</p>\n<p>What was profound was that the lack of relationship I had with the stuff I just sold on eBay actually amplified the deep relationship I feel with the La Pavoni.</p>\n<p><strong>Tools I use often in the kitchen</strong></p>\n<p>I'd been thinking about tools I use often, since <a href=\"https://twitter.com/qmacro/status/1374655713331544065\">noticing how worn my hand milk frother was recently</a>.</p>\n<img src=\"https://qmacro.org/content/images/2021/03/milkfrother.jpeg\" alt=\"Worn milk frother\" width=\"200\" height=\"138\" />\n<p>I've had that milk frother for about 10 years. I've had a moka pot for about that long too - originally one from Bialetti, which I eventually replaced with one from IKEA (which is surprisingly excellent).</p>\n<img src=\"https://qmacro.org/content/images/2021/03/mokapot.png\" alt=\"IKEA RÅDIG moka pot\" width=\"147\" height=\"155\" />\n<p>And I've had the La Pavoni Professional Lusso for almost 2 years.</p>\n<img src=\"https://qmacro.org/content/images/2021/03/lapavoni.jpeg\" alt=\"La Pavoni and espresso cup\" width=\"300\" height=\"400\" />\n<p>Give or take, I've used each of these items <strong>every single day</strong> since I've had them. Often more than once per day. (In case you're wondering, I make M's latte with the moka pot and froth the milk manually, as that's how she prefers it, and I make my espresso with the La Pavoni).</p>\n<p>These are just examples of course, but they're very visceral because I use all of them with my hands and what they produce is also consumed by me and M.</p>\n<p>There's something special about tools like this. The bond, the attachment, the relationship that builds is more something than nothing. Anyway, before I get too philosophical, I'll get to the other half of this post, which is about tools I use at work.</p>\n<p><strong>Tools I use often at work</strong></p>\n<p>I like the command line. Give me a terminal over a GUI any day. The command line is a rich and powerful environment because of the expressive nature and the closeness you feel to the things you're trying to do (or manipulate).</p>\n<p>That power comes from the combination of two things, the shell, and the commands available to you in your path (for more on the shell, see <a href=\"https://qmacro.org/autodidactics/2020/12/28/waiting-for-jobs/\">Waiting for jobs, and the concept of the shell</a>).</p>\n<p>Without thinking too hard, here's a list of commands, of tools, that I use in the context of the shell, every single day:</p>\n<ul>\n<li><code>vim</code> (editor)</li>\n<li><code>tmux</code> (terminal multiplexer)</li>\n<li><code>curl</code> (HTTP client)</li>\n<li><code>fzf</code> (fuzzy finder)</li>\n<li><code>jq</code> (JSON processor)</li>\n</ul>\n<p>(One could say that the combination of <code>vim</code>, <code>tmux</code> and the shell is my IDE.)</p>\n<p>Of course, I use other commands too, and many Bash shell builtins &amp; features, but I'd say these are tools that I find essential.</p>\n<p><strong>More learning required</strong></p>\n<p>As well as being daily drivers, regardless of the task at hand, what else do these tools have in common?</p>\n<p>Well, to be honest - there's still much that I don't know about them.</p>\n<p>In many ways, one could argue that these tools represent the zenith of achievement in their area:</p>\n<ul>\n<li>there are few editors as powerful or accomplished as <code>vim</code></li>\n<li><code>tmux</code> is the de facto standard for managing terminal sessions</li>\n<li><code>curl</code> is possibly the most popular HTTP client mechanism out there, in command line tool form as well as in library form</li>\n<li>someone <a href=\"https://lobste.rs/s/nsfdaw/improving_shell_workflows_with_fzf#c_2um216\">said this</a> about <code>fzf</code> recently, and I tend to agree: &quot;<em>I don’t think any other single cli tool has ever had such a big and positive impact on my workflow than fzf has, it’s really a great piece of work</em>&quot;.</li>\n<li>while there are other great options such as <code>fx</code>, it's <code>jq</code> that everyone turns to, to handle JSON data on the command line</li>\n</ul>\n<p>So while at least the La Pavoni machine has moving parts, it's still a block of stone compared to these tools, which all have such rich and varied features.</p>\n<p>Here are a few example of what I've only recently discovered, or perhaps uncovered, with these tools.</p>\n<ul>\n<li>I managed to write some Vimscript to call <a href=\"https://github.com/mvdan/sh\"><code>shfmt</code></a> to pretty-print my shell scripts on save</li>\n<li>I <a href=\"https://qmacro.org/autodidactics/2021/04/01/new-tmux-panes-and-windows-in-right-dir/\">worked out</a> how to get <code>tmux</code> to open a new window or pane in the same directory as I was when I invoked the open command</li>\n<li>Having read <a href=\"https://seb.jambor.dev/posts/improving-shell-workflows-with-fzf/\">Improving shell workflows with fzf</a> I learned about how to configure my own previews</li>\n<li>I remembered (I'd forgotten) that I can use <code>--data-urlencode</code> to have values automatically URL encoded with <code>curl</code></li>\n<li>I'm only just now starting to feel comfortable enough to embrace <code>jq</code> as a complete language, with my <a href=\"https://github.com/qmacro/dotfiles/blob/master/scripts/dwr#L21-L38\">first script with function definitions</a></li>\n</ul>\n<p>As those lovely folks that join my live stream sessions* know - I'm not afraid of admitting that &quot;I've no idea what I'm doing&quot;.</p>\n<p>*I live stream usually weekly on Friday mornings UK time - look for the Hands-on SAP Dev episodes on the <a href=\"https://www.youtube.com/channel/UCNfmelKDrvRmjYwSi9yvrMg\">SAP Developers</a> YouTube channel.</p>\n<p>At the beginning of last year, along with other folks in the <a href=\"https://community.sap.com/\">SAP Community</a>, I wrote up <a href=\"https://blogs.sap.com/2020/01/12/my-learning-list-for-2020/\">my learning list for 2020</a>. In it, I had a section titled &quot;Understanding core things better&quot;, and while that contained the kernel of the idea that I want to improve my understanding of fundamental things, I think I missed the mark somewhat. I failed to spot the tools that were right in front of me (or my fingers).</p>\n<p>So I guess this is a reminder for me that I'm nowhere near done. That's fine, continuous learning is a thing, and as it is for many others, it's my thing.</p>\n<p>Triggered by some mundane moments recently (eBay activities, gasket maintenance, the wearing thin of a simple wooden handle), I've come to realise what I need to do. And that is far from mundane. It won't be a short process -- I think mastery of these tools will only come over years -- but the journey will enjoyable and rich from the outset.</p>\n",
      "date_published": "2021-03-31T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-03-27-la-pavoni-maintenance-successful/",
      "url": "https://qmacro.org/blog/posts/2021-03-27-la-pavoni-maintenance-successful/",
      "title": "La Pavoni maintenance successful",
      "content_html": "<p><em>I've successfully carried out maintenance on my La Pavoni espresso machine, and you can too. Here are some notes that may help.</em></p>\n<p>I'd been slightly apprehensive about replacing the gaskets on my La Pavoni lever espresso machine, as I'm not particularly skilled at this kind of thing and didn't want to break anything. But I've just gone through the process and things seem to have worked out OK, and I wanted to share that information - because if I can do it, you can too.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/03/lapavonimaintenance.jpeg\" alt=\"My La Pavoni in mid-maintenance\" /></p>\n<h2 id=\"why-i-did-it\" tabindex=\"-1\">Why I did it <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-03-27-la-pavoni-maintenance-successful/#why-i-did-it\" aria-hidden=\"true\">#</a></h2>\n<p>I'd started to notice some leakage in the grouphead - once the machine was up to temperature and pressure, water would drip out into the drip tray (or cup). Sometimes only a few drops; but at other times almost up to an espresso cup's worth.</p>\n<p>Related, I'm sure, was the fact that the lever action, both up and down, was far from smooth. It was, how can I put it, bumpy and uneven, as though something was rubbing or catching. The odd thing is that because this happened over time, I didn't actually notice it in the early stages. But it became quite severe and I guessed it was a gasket issue - which was very likely related to the leakage too.</p>\n<h2 id=\"learning-how-to-do-it\" tabindex=\"-1\">Learning how to do it <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-03-27-la-pavoni-maintenance-successful/#learning-how-to-do-it\" aria-hidden=\"true\">#</a></h2>\n<p>I watched and re-watched various videos on YouTube, and found that this one from Sam Stiles was the most helpful: <a href=\"https://www.youtube.com/watch?v=DwLkxOpXSOg\">La Pavoni- 5 piece gasket replacement</a>. I must have watched that one at least 7 or 8 times before I started.</p>\n<h2 id=\"what-i-ordered\" tabindex=\"-1\">What I ordered <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-03-27-la-pavoni-maintenance-successful/#what-i-ordered\" aria-hidden=\"true\">#</a></h2>\n<p>I ordered a <a href=\"https://www.theespressoshop.co.uk/en/La-Pavoni-Lever-Grouphead-Service-Kit-New-Group/m-2791.aspx\">La Pavoni Lever Grouphead Service Kit (New Group)</a> from The Espresso Shop. I chose this to order as I wasn't sure what I needed, and in fact it came with some parts that were a nice bonus (more on that shortly).</p>\n<p>Following the process demonstrated very ably by William Stiles in the video, I managed to remove the lever and then the grouphead, and then the piston.</p>\n<h2 id=\"tools\" tabindex=\"-1\">Tools <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-03-27-la-pavoni-maintenance-successful/#tools\" aria-hidden=\"true\">#</a></h2>\n<p>Having watched how William used his tools, I ordered a <a href=\"https://www.amazon.co.uk/gp/product/B0818WLSGS/\">set of snap ring pullers</a> to be able to remove the snap ring holding the piston shaft gasket in place.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/03/snapringpullers.png\" alt=\"Snap ring pullers\" /></p>\n<p>Having seen one used in another video, I also ordered a cheap <a href=\"https://www.amazon.co.uk/gp/product/B00G3CCJ94/\">hook and pick set</a>. I wasn't entirely sure I'd need them but in fact they were very handy to have - I found the 90 degree angled one useful for prying the old gaskets off, and for retrieving the aforementioned piston shaft gasket.</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/03/hookandpickset.png\" alt=\"Hook and pick set\" /></p>\n<p>I already had a rubber mallet so with the arrival of those online purchases, I was all set.</p>\n<h2 id=\"observations\" tabindex=\"-1\">Observations <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-03-27-la-pavoni-maintenance-successful/#observations\" aria-hidden=\"true\">#</a></h2>\n<p>I went slowly and carefully, and everything pretty much went as William described. Here are some observations and experiences that I thought might be helpful.</p>\n<p><strong>Lever attachment</strong></p>\n<p>The lever attachment section was pretty dirty - especially the bolts and the roller nut; while I cleaned the lever itself, and the top of the grouphead, I didn't need to clean the bolts or the roller nut, as the service kit included new ones, and also new clips (called &quot;circlips&quot;, which was new to me) that hold the bolts in place.</p>\n<p><strong>Removing the piston</strong></p>\n<p>It was quite difficult to remove the piston from the grouphead; I whacked it with the rubber mallet just like William did, with some force, but it wasn't budging.</p>\n<p>But after removing the large gasket at the bottom of the grouphead, the one holding the shower screen in (i.e. the one that the top of the portafilter touches when you attach it when about to pull a shot) ... one more whack with the mallet did it and it came out much more easily.</p>\n<p>With it came a small amount of water - luckily, I'd placed a towel where the drip tray usually goes, not for any water, but to prevent the piston scratching the chrome as it shot out of the bottom.</p>\n<p><strong>Cleaning the piston</strong></p>\n<p>I made sure to clean each and every part I could get access to, but the piston itself was by far the most grimy. I spent about 10 minutes with some hot soapy water and a gentle panscrub to remove a layer, which was quite greasy (from the coffee oils, I guess).</p>\n<p>This was after removing the old piston gaskets, which looked pretty knackered. I found the pick set was useful for this, by the way.</p>\n<p><strong>Replacing the piston gaskets</strong></p>\n<p>After removing the piston, I found that this was one of the hardest things to do. It was easy enough to pry the old gaskets off, but the new ones weren't for going on easily. In another video, I saw that the person had pre-soaked the gaskets in warm water for 5 minutes to make them a little more pliable.</p>\n<p>Doing this helped, but it still took me a couple of attempts. First of all I managed to get one on, but when I looked, it had twisted around and the &quot;groove&quot; was facing outwards. So I had to remove it and try again.</p>\n<p>Removing it was difficult too, I didn't want to use the pick, or even a flathead screwdriver as they both had sharp bits and I didn't want to damage the new gaskets. But then I realised I could use the thin end of a teaspoon. Smooth, and, as it turns out, ideal!</p>\n<p>I made sure I put the gaskets on as William had instructed, i.e. like the shape of a guitar body.</p>\n<p><strong>The piston shaft gasket, washer and snap ring replacement</strong></p>\n<p>Getting to the piston shaft gasket was fiddly but doable - mostly thanks to one of the snap ring pullers in the set I bought. Definitely recommended. I've no idea how I would have removed the snap ring without it, and I'm pretty certain I would have had no chance to put it back either.</p>\n<p>By the way, the service kit also included a new snap ring, plus the washer that sits between the ring and the gasket. That was nice, as both the ring and washer were quite dirty and some limescale had built up there too. The gasket itself was pretty decrepit, at least as worn as the piston gaskets, if not worse.</p>\n<p><strong>Re-inserting the piston</strong></p>\n<p>When I was ready to re-insert the piston, I made sure to lightly grease the piston itself (around the head, including the gaskets), and put a small amount on the shaft. This helped with the insertion, but only after I'd fiddled around with a spoon to squash in the flanged part of the topmost piston gasket so it would go back into the grouphead.</p>\n<p>You see William doing this, but as he did it so deftly, I didn't notice at first. It took me a minute or so to get this done.</p>\n<p><strong>Re-attaching the lever</strong></p>\n<p>Once the piston was in, the lever re-attachment was pretty easy. One thing I found fascinating is that the lower of the two nuts that are screwed onto the top of piston shaft is to provide an appropriate &quot;stop&quot; point so that the piston doesn't go too low. I found that I had to adjust that nut a little bit as, later, when I attached the portafilter, the lever handle was touching the portafilter handle.</p>\n<p>One thing that was lovely to see - and feel - was the huge difference this maintenance made. In both directions, the lever action (and more importantly piston action) was totally smooth, I couldn't believe how much better it was.</p>\n<p><strong>Re-attaching the shower screen and filter holder gasket</strong></p>\n<p>The final task was to re-attach the screen at the bottom of the grouphead. I say &quot;re-attach&quot;, but in fact the service kit also came with a new one, so I used that.</p>\n<p>Regarding the gasket - there were two in the service kit - one thinner one with a round cross-section, and one fatter one with a U cross-section. I guessed (correctly, I think) that it was the fatter one that I needed, based on the size of the old one that I'd removed.</p>\n<p>The idea is that you slide it over the screen, from the bottom of the screen up to the lip, first, and then insert both into the bottom of the grouphead.</p>\n<p>But as this fatter one had a U cross-section, there was a chance that I'd slide it up and insert it the wrong way.</p>\n<p>And I did.</p>\n<p>First of all I'd put it in with the &quot;flat&quot; part (the top of the U) facing upwards up into the grouphead. However, I couldn't get the portafilter in. After a few moments head scratching, I realised that the gasket must be the wrong way round and that the top of the U was preventing it from being pushed far enough up.</p>\n<p>I pulled it gently back out (using the smooth edge of the teaspoon again) and slid it back over the screen, this time so that the flat part of the U was facing downwards (like this: ∩) and would come into contact with the top of the portafilter when inserted.</p>\n<p>This was much better, and I could use the portafilter (without the basket, as William demonstrated) to push the screen and gasket up into the grouphead.</p>\n<p>Update: I realised after posting this, and re-examining the schematic diagram that came in the service kit, that the &quot;other&quot; gasket was perhaps not an alternative filter holder gasket, but a group sleeve gasket - item number 77 in the schematic (see later for a section of that diagram). The group sleeve is the light-coloured plastic or bakelite cylinder inside of the grouphead, inside which the piston moves. I couldn't get this off as I didn't have the right tool, but I'd decided that that was fine, I'd do that next time. Anyway, I think I now realise what this &quot;spare&quot; gasket is for.</p>\n<p><strong>The group to boiler gasket</strong></p>\n<p>I didn't forget to replace the gasket between the grouphead and the boiler itself, that was the easiest part. It's important to note here, however, that I heeded William's advice not to over-tighten the two bolts that hold the grouphead onto the boiler. On tightening them, I came to feel a natural &quot;stop&quot; and didn't apply any further torque.</p>\n<h2 id=\"conclusion\" tabindex=\"-1\">Conclusion <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/2021-03-27-la-pavoni-maintenance-successful/#conclusion\" aria-hidden=\"true\">#</a></h2>\n<p>The entire process took about 90 mins, as I was going very slowly and also re-watching parts of the video as needed. I'm not very dextrous but managed to complete the service successfully.</p>\n<p>I thought this experience and process was worth sharing, especially for folks that might be in my position right now - thinking or knowing you need to do it but being a little apprehensive.</p>\n<p>It's doable, and definitely worth it!</p>\n<p><img src=\"https://qmacro.org/blog/img/2021/03/schematic.png\" alt=\"La Pavoni schematic diagram\" />.</p>\n",
      "date_published": "2021-03-27T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-03-25-a-new-journal-experiment---thinking-aloud/",
      "url": "https://qmacro.org/blog/posts/2021-03-25-a-new-journal-experiment---thinking-aloud/",
      "title": "A new journal experiment - Thinking Aloud",
      "content_html": "<p><em>I'm trying out a new way of sharing thoughts in a GitHub issues based journal style blog called <a href=\"https://github.com/qmacro/thinking-aloud\">Thinking Aloud</a>.</em></p>\n<p>TL;DR - My <a href=\"https://github.com/qmacro/thinking-aloud\">Thinking Aloud repo</a> is where I am experimenting with journalling via GitHub issues. Check out the <a href=\"https://github.com/qmacro/thinking-aloud/issues?q=is%3Aissue+is%3Aopen+label%3Aentry\">issues themselves</a>, the <a href=\"https://github.com/qmacro/thinking-aloud/blob/main/recent.md\">rendered versions of recent entries</a>, the <a href=\"https://raw.githubusercontent.com/qmacro/thinking-aloud/main/feed.xml\">Atom feed</a> or the <a href=\"https://github.com/qmacro/thinking-aloud/tree/main/.github/workflows\">GitHub Actions workflows</a> with which I automate some of the process.</p>\n<p>I've been blogging for over 20 years, since 2000. I started with a Blogspot hosted blog over at <a href=\"https://qmacro.blogspot.com/\">https://qmacro.blogspot.com</a> which amazingly is still around.</p>\n<p><strong>This blog</strong></p>\n<p>I quickly moved over to a self-hosted blogging system, initially powered by the beautifully simple <a href=\"https://en.wikipedia.org/wiki/Blosxom\">Bloxsom</a>.</p>\n<p>Over the years I tried out various blogging software, including Ghost and Movable Type, but at the core I've had my main blog (now at <a href=\"https://qmacro.org/\">https://qmacro.org</a> - where you're reading this) since 2002. I'm currently using GitHub Pages to host and manage things and I'm quite happy with it. I see this blog as my main personal blog and a place for &quot;long form&quot; posts on various subjects (as you can see from the <a href=\"https://qmacro.org/\">index</a>).</p>\n<p><strong>My posts on the SAP Community blog</strong></p>\n<p>Of course, I also publish on the <a href=\"https://community.sap.com/\">SAP Community</a> blog, which is a collective set of posts by many, many members of the SAP ecosphere. I have posts under the <a href=\"https://people.sap.com/dj.adams\">dj.adams</a> identifier and also (since I joined SAP) under the <a href=\"https://people.sap.com/dj.adams.sap\">dj.adams.sap</a> identifier, and as you might expect, the subject matter is very definitely SAP related. That said, you may be surprised at the breadth of topics - posts on subjects as diverse as <a href=\"https://blogs.sap.com/tag/terminaltip/\">terminal tips</a> and <a href=\"https://blogs.sap.com/2020/12/01/utfrw-unofficial-teched-fun-run-walk/\">fun runs</a> are all there.</p>\n<p><strong>My autodidactics blog</strong></p>\n<p>In the middle of last year I started a new, secondary blog <a href=\"https://qmacro.org/autodidactics/\">autodidactics</a> to share things I'd learned (I endeavour to be a life long learner). I was inspired to create such a blog having seen <a href=\"https://til.simonwillison.net/\">Simon Willison's TIL (Today I Learned)</a> site.</p>\n<p>Moreover, I did very definitely feel I needed a place to share smaller nuggets of information that I'd learned; this in turn was triggered by reading some of <a href=\"https://rwxrob.live/\">rwxrob</a>'s repository of <a href=\"https://github.com/rwxrob/dotfiles/\">dotfiles and scripts</a>.</p>\n<p>Ever since I read through the entire source code base of the original Jabber (XMPP) server <code>jabberd</code> to understand how everything worked, in researching for my O'Reilly book <a href=\"https://qmacro.org/about/#writing-and-talks\">Programming Jabber</a>, I've been a strong proponent of reading other people's code. There's so much richness out there, a variety of styles and approaches, and oh so much to learn.</p>\n<p><strong>Twitter</strong></p>\n<p>And of course, when it comes to sharing thoughts, there's always <a href=\"https://twitter.com/qmacro\">Twitter</a>, which has been referred to as a &quot;microblogging&quot; platform, in the same way that <a href=\"https://wiki.p2pfoundation.net/Identica\">identi.ca</a> was. The key difference between Twitter and <a href=\"http://identi.ca/\">identi.ca</a> was that the former is centralised, and the latter (sadly no longer in operation) was distributed. With <a href=\"http://identi.ca/\">identi.ca</a> I felt in more control of my microblogging efforts. Don't get me wrong, Twitter is a great platform for conversation and ideas, but it's still centralised.</p>\n<p><strong>Journalling</strong></p>\n<p>And so to <a href=\"https://github.com/qmacro/thinking-aloud\">Thinking Aloud</a>. If I lay out the different outlets for my thoughts in decreasing order of magnitude, I end up with something that looks like this:</p>\n<pre><code>+---------------------------------------------------------------+\n|    Major      |    Minor      |     Mini      |     Micro     |\n|---------------|---------------|---------------|---------------|\n|  qmacro.org   | autodidactics |  (something   |    Twitter    |\n| SAP Community |               |   missing)    |               |\n+---------------------------------------------------------------+\n</code></pre>\n<p>What do these categories mean to me?</p>\n<p><strong>Major</strong>: If I want to write something in the major category, that's a relatively significant investment in time to create and publish posts (and for the consumer it can be significant too). That's fine, and those posts definitely will always have their place.</p>\n<p><strong>Minor</strong>: If I want to share something specific that I learned, such as on the subject of the shell's <code>declare</code> builtin (in <a href=\"https://qmacro.org/autodidactics/2020/10/08/understanding-declare/\">Understanding declare</a>), I have my <a href=\"https://qmacro.org/autodidactics\">autodidactics</a> blog. The posts are usually shorter -- although some may be more densely packed -- and about something quite small and specific.</p>\n<p><strong>Micro</strong>: If I just want to share a fleeting idea (or rant), I have <a href=\"https://twitter.com/qmacro\">Twitter</a>.</p>\n<p>So I feel there's a gap, for the <strong>Mini</strong> category. I have been inspired by <a href=\"https://rwxrob.live/\">rwxrob</a>'s journalling, where he writes in relatively short form, but in a structured fashion. It seems a way of getting things written down, freeing up mental space for new ideas, and also a semi-cathartic approach to expressing thoughts, regardless of how fully formed (or not) they are.</p>\n<p>One of the aspects that I like about the journalling that I've seen is that it's about the body of the journal entry first, and the title is not important. In fact, rwxrob's journal titles are timestamps, which seems a great way to avoid wasting brain cycles trying to think of a title, either before writing the entry (when you don't exactly know what you're going to write), or after (when you may have covered various topics in one entry).</p>\n<p>So I've decided to try to feel my way into this <strong>Mini</strong> gap, and do some journalling of my own. The idea is that the amount of pre-thought, the level of friction &amp; inhibition to create a new journal entry should reflect where this is in the &quot;scale&quot; expressed in the table above. I don't think much before tweeting (maybe I should, but that's a different story) and journalling is more towards that end of the scale than the other.</p>\n<p><strong>Using GitHub features</strong></p>\n<p>As part of the experiment, I decided to learn more about GitHub features while doing this, by making them a fundamental basis for the journalling mechanism.</p>\n<p>I have a new GitHub repository <a href=\"https://github.com/qmacro/thinking-aloud\">thinking-aloud</a>, and each journal entry is an <a href=\"https://github.com/qmacro/thinking-aloud/issues\">issue</a> in there. The beauty of GitHub issues is that Markdown is supported, plenty rich enough to express my ideas.</p>\n<p>Moreover, there are other metadata aspects such as labels that I might want to take advantage of at some stage (think <a href=\"https://github.com/qmacro/thinking-aloud/blob/08bf3f98064237c35b3bf7ae4fb16b5ecb9608b6/feed#L44\">&quot;categories&quot; in Atom feed entries</a>).</p>\n<p>Not least is the chance for folks to engage with the journal entries, via reactions and comments. I'm not sure how this is going to pan out, but I want to at least give this aspect a chance. I may get no engagement, I may get a load of spam. Let's see.</p>\n<p>Most interestingly (to me) is the way I create new journal entries, and how I build the Atom feed so folks can subscribe.</p>\n<p>I create a new entry via a shell function <code>j</code>, at the heart of which is this invocation:</p>\n<pre class=\"language-shell\"><code class=\"language-shell\">gh issue create --title <span class=\"token string\">\"<span class=\"token variable\"><span class=\"token variable\">$(</span><span class=\"token function\">date</span> <span class=\"token string\">'+%Y-%m-%d %H:%M:%S'</span><span class=\"token variable\">)</span></span>\"</span></code></pre>\n<p>My editor (Vim) is then launched and I write Markdown, which is then sent to be the body of a new issue when I finish. Simple!</p>\n<p>Each time a new journal entry (issue) is created, I rebuild the Atom feed. This is done via the power of GitHub Actions. Have a look at the <a href=\"https://github.com/qmacro/thinking-aloud/blob/main/.github/workflows/generate-feed.yml\">generate-feed workflow</a> to get an idea of how that works; in one of the steps there, I'm using <code>gh</code> to call the GitHub API to get the list of issues, and piping that (JSON) into a simple Node.js script <a href=\"https://github.com/qmacro/thinking-aloud/blob/main/feed\">feed</a> that uses the freakishly easy-to-use NPM module <a href=\"https://www.npmjs.com/package/feed\">feed</a> (thanks <a href=\"https://github.com/jpmonette\">jpmonette</a>!) to generate the Atom feed.</p>\n<p>Additionally, I have implemented some <a href=\"https://github.com/qmacro/thinking-aloud/pull/5/files\">simple rendering</a> to make the entries easier to consume - the <a href=\"https://github.com/qmacro/thinking-aloud/blob/main/recent.md\">most recent entries are rendered into a Markdown file</a> in the main repository, and GitHub's Markdown rendering is more than good enough to make things easy and pleasant to read.</p>\n<p><strong>Summary</strong></p>\n<p>And that's it, so far. As usual, I'm making this up as I go along, and things may change along the way. I've written a couple of journal entries already, <a href=\"https://github.com/qmacro/thinking-aloud/blob/main/recent.md\">check them out</a> and let me know what you think.</p>\n",
      "date_published": "2021-03-25T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-02-02-thoughts-on-video-content/",
      "url": "https://qmacro.org/blog/posts/2021-02-02-thoughts-on-video-content/",
      "title": "Thoughts on video content",
      "content_html": "<p><em>Here are some thoughts on video content - form, length, frequency and more, especially in a learning and sharing context, and specificially in a developer advocate content.</em></p>\n<p>Just now my good friend Ronnie Sletta <a href=\"https://twitter.com/rsletta/status/1356563739676471297\">drew our attention to</a> a question by <a href=\"https://chrismroberts.com/\">Chris Roberts</a> on video content: &quot;<em>I’m thinking of making some free courses and putting them on YouTube. Do you prefer a series of short videos, or one long video? If a series, should I release one a week? If one long video, how long?</em>&quot;.</p>\n<p>I started to reply on Twitter, then found myself needing to use the &quot;1/n&quot; tweet thread approach, which I've never really liked, so I thought I'd take a <a href=\"https://www.hanselman.com/blog/your-blog-is-the-engine-of-community\">leaf out of Scott Hanselman's book</a> and reply once in a form that's arguably more permanent and easier to read.</p>\n<p><strong>Form variation</strong></p>\n<p>First off, I think it's important for us to remember that folks have different preferences when it comes to consuming content and learning. Some prefer the written word, and some prefer video instruction or demonstration. But it goes deeper than that.</p>\n<p>Descend into the video category and within that there are some that like to settle in and watch a &quot;feature length&quot; video. Others prefer short, sharp and to-the-point videos, the moving picture equivalent of good answers on Stack Overflow. Then there's the question of time available, which also factors into the decision on which videos to watch. Often I find myself thinking:</p>\n<p>&quot;<em>OK, I've got 15 mins before the next meeting, and I'd like to learn more about X - what videos fit that combination of time and topic filter?</em>&quot;</p>\n<p>It seems obvious to me that when it comes to the time filter, shorter time slots are going to be more common, so in one sense shorter videos in general seem like a good idea.</p>\n<p>But there are cases to be made for both short and long form videos. They serve different purposes and contexts.</p>\n<p><strong>Long form</strong></p>\n<p>Before I proceed with this thought, let me be clear on what I mean by &quot;long form&quot;. There are recordings of live streams from YouTube or Twitch sessions that are very often far more than an hour or so. A live stream that is just one hour is actually quite unusual.</p>\n<p>For me though, an &quot;hour&quot; is long form. Anything longer than that is beyond the question at hand, and for me fits into the &quot;<em>I want to watch my favourite streamer and relax, so I'll watch this recording as they're not live right now, or because it covers something I'm interested in</em>&quot; category.</p>\n<p>It's worth pointing out at this point that the <a href=\"https://www.youtube.com/playlist?list=PL6RpkC85SLQABOpzhd7WI-hMpy99PxUo0\">main live stream episodes that I put out</a> are deliberately limited to one hour. That's for many reasons, here's the main one:</p>\n<ul>\n<li>the folks that join have other things to do in their day, meetings to go to, and so on; I don't want to make it harder for them to do that, and by streaming from the top of the hour to the top of the next hour makes it easier for them to fit the live stream attendance into their work calendar</li>\n</ul>\n<p>This applies to me too - I have other tasks to accomplish and meetings to attend in my working day.</p>\n<p>With this in mind, I'd say that the &quot;long form&quot; categorisation of one hour videos only applies if they're not live. I'd almost go so far as to say that a live stream of less than an hour has more disadvantages than advantages:</p>\n<ul>\n<li>the time to establish the topic and focus is limited</li>\n<li>some folks joining don't get enough time to &quot;warm up&quot; and feel confident enough to participate in the chat</li>\n<li>given that a live stream has a fixed broadcast start time, if you miss it, you miss it, so a longer live stream gives latecomers (not anything to be ashamed of!) chance to actually catch something, participate and get something from it</li>\n</ul>\n<p><strong>Video chapters</strong></p>\n<p>While short form videos are great for focused search, confirmation and consumption, there's a key feature that's essential for making longer form videos more easily consumable and more useful, and that's the <a href=\"https://support.google.com/youtube/answer/9884579\">video chapters</a> feature.</p>\n<p>I think they go some considerable way towards bringing the &quot;long form&quot; videos closer to &quot;short form&quot; in consumability and relevance.</p>\n<p>I've been making use of video chapters for a while, and I can thoroughly recommend it.</p>\n<p><strong>Some examples</strong></p>\n<p>I use video chapters in my live stream recordings; after a live stream is finished, I scrub through the recording and then add video chapter information in the description, making it much more accessible and useful for consumers. Here's an example from an episode of the <a href=\"https://www.youtube.com/playlist?list=PL6RpkC85SLQCBncEWbkHTLz7ykB9C7yof\">Getting the most out of the SAP TechEd Developer Keynote repository</a> series on the <a href=\"https://www.youtube.com/playlist?list=PL6RpkC85SLQABOpzhd7WI-hMpy99PxUo0\">Hands-on SAP Dev</a> show*.</p>\n<p>*<em>See <a href=\"https://blogs.sap.com/2020/11/09/an-overview-of-sap-developers-video-content/\">An overview of SAP Developers video content</a> for more details.</em></p>\n<p><img src=\"https://qmacro.org/blog/img/2021/02/videochapters.png\" alt=\"video chapters in the description\" /></p>\n<p>My other SAP Developer Advocate colleagues use video chapters too.</p>\n<p>Talking of <a href=\"https://youtube.com/sapdevs\">SAP Developers video content</a>, we've just launched the first video in a new shorter form show - SAP Tech Bytes. The videos here are deliberately short, to be more consumable in a shorter amount of time, to be focused on one specific topic, and also to provide form variation.</p>\n<p>Here's the first episode: <a href=\"https://www.youtube.com/watch?v=O0x7Jt6yre0\">SAP Tech Bytes: Tutorial - Create SAP HANA Database Project</a>. Note that there are video chapters even in this shorter form content.</p>\n<p>In fact, we use video chapters in even our shortest form videos - the episodes of the <a href=\"https://www.youtube.com/playlist?list=PL6RpkC85SLQAVBSQXN9522_1jNvPavBgg\">SAP Developer News</a> show, where each video is only around five minutes long - deliberately a coffee break length.</p>\n<p><strong>Frequency and schedule</strong></p>\n<p>I've just got to here and realised I haven't even talked about frequency. I thought it might be at least helpful to give some examples; I've been live streaming in my role as SAP Developer Advocate since January 2019, and have kept more or less the same frequency since then, which is weekly. I've done the occasional second live stream in a single week, but I treat (and refer to) those as &quot;off piste&quot; and not really part of the usual cadence.</p>\n<p>Perhaps more importantly than the frequency, at least for live streams, is the consistency of day and time, i.e. the schedule. I think of my episodes of Hands-on SAP Dev as episodes of a TV programme, and again, because it's broadcast live, the best way to help folks <em>not</em> to miss it is to be consistent and predictable with the schedule. That's why I broadcast my episodes on Fridays, at 0800 GMT.</p>\n<p>Incidentally, I chose a relatively early morning slot because that's when I'm most awake and my brain is buzzing - I'm a morning person and the plate spinning that's required to stream live is slightly less difficult then.</p>\n<p>I think the same scheduling rules apply to YouTube <a href=\"https://www.youtube.com/watch/Wbi_Ic1DDwQ\">Premieres</a> too. Premieres are videos that you can pre-record but have the first broadcast set for a fixed date and time in the future, with all the build up and excitement of a live stream, but during playback you can attend and interact in the chat with the viewers. Sort of a combination of live stream and recording, which can work really well.</p>\n<p>When it comes to recorded videos, i.e. neither live streams nor premieres, then the schedule is not that important, so it just comes down to frequency. And that really depends on two factors:</p>\n<ul>\n<li>primarily: how much content you can produce</li>\n<li>secondarily: how much content your viewers can consume</li>\n</ul>\n<p>There's a balance you need to find between these two factors, and there's no algorithm I know of that will provide a solid answer here; it really depends on how you work, what you have to share, and so on.</p>\n<p>That said, here's a general piece of advice: If you have a load of pre-recorded videos waiting to upload to YouTube, don't be tempted to just publish them all at once. Resist the urge to flood your viewers' brains with all that wonderful goodness, and instead publish them in a spaced-out fashion instead. That has two advantages:</p>\n<ul>\n<li>they don't feel overwhelmed and tempted to give up because there's too much to consume</li>\n<li>you don't burn yourself out, and instead give yourself time to think of other content</li>\n</ul>\n<p><strong>Wrapping up</strong></p>\n<p>Anyway, this post is already far longer than I expected it to be; I'll bring it to a close now, but as it's just blog post content, I may come back to it in the future and update it as I see fit. That's the wonderful nature of blog posts and how they're still the backbone of many communities.</p>\n<p>Happy videoing!</p>\n",
      "date_published": "2021-02-02T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-02-01-do-less-and-do-it-better/",
      "url": "https://qmacro.org/blog/posts/2021-02-01-do-less-and-do-it-better/",
      "title": "Do less and do it better",
      "content_html": "<p><em>In 2021 I want to consolidate and improve upon some skills I already have, rather than add more. Here's what I mean, and how I got inspired.</em></p>\n<p>In October last year Samir Talwar <a href=\"https://twitter.com/SamirTalwar/status/1318904227935227905\">tweeted</a> something simple yet profound: &quot;<em>Do less, and do it better</em>&quot;.</p>\n<p>In my work and play I discover and start using various tools and technologies. The pace of change in this industry, coupled with the (not unpleasant) demands on what I have to produce, means that I often end up with only a shallow understanding of things. And sometimes these are things I use every day.</p>\n<p>The nature of my job as a developer advocate (but I think this extends to development in general), in the context of that fast pace of change, means that there's always something new to learn, to adopt, and to incorporate into a workflow, process or solution. But that can come at a price - of limited comprehension and mastery.</p>\n<p>To explain further, I'm going to stretch a metaphor relating to ploughing a field and sowing seeds.</p>\n<p><strong>Ploughing and sowing</strong></p>\n<p>As an individual, I sometimes feel as though I'm trying to prepare a large field and plant seeds there using a poorly hand-constructed and inefficient plough made of the wrong sort of wood and bits of string, combined with a seed drill made out of old toilet rolls and sticky tape. Not only that, but I'm trying to plant across the entire field, 50 furrows wide, as I move along.</p>\n<p>Needless to say, the ploughing doesn't go very well, and the seeds are planted imprecisely, sometimes superficially, mostly wastefully, resulting in poor distribution, low growth and high energy expenditure.</p>\n<p>But if I were to abandon the idea of going wide, and instead go narrow, focusing on just a handful of furrows, I could afford to take the time to correctly plant each seed, nurturing &amp; watering each one, producing strong plants with deep roots and healthy growth.</p>\n<p>I've thought this for a while but never got round to doing anything about it. Samir's tweet has galvanised me into spending some time working out what that means for me.</p>\n<p><strong>Consolidating</strong></p>\n<p>So this year I'm attempting to &quot;do less, and do it better&quot; by acknowledging the tools I use day in day out, and learn more about them, restricting myself to a narrow set of topics, move a step closer towards mastery in each, and really benefit from everything they have to offer.</p>\n<p>Here's an example from this weekend; I read the entirety of the main README for the excellent fuzzy-finder tool <a href=\"https://github.com/junegunn/fzf\"><code>fzf</code></a>, all 16 pages. That might seem ridiculous to say (16 pages is not a lot) but I've used <code>fzf</code> for a year or so and never RTFM'd before. In my defence, I've also been constantly and painfully aware that I've merely scratched the surface. I've now discovered some <code>fzf</code> gems that I can put into practice immediately, and some areas that I need to dig into more.</p>\n<p>Likewise for other tools that I use, tools that are not only essential, but which, when mastered, can make my workflows even better. I'm thinking of Vim (I've recently started watching my friend and colleague David Kunz's <a href=\"https://www.youtube.com/channel/UCFU7a7OMYfcpjtIpu2j47_Q\">DevOnDuty</a> series, which I can strongly recommend), <a href=\"https://github.com/tmux/tmux/wiki\"><code>tmux</code></a> (<a href=\"http://rwxrob.live/\">rwxrob</a> is a great practitioner, and I should re-read Brian P. Hogan's great <a href=\"https://pragprog.com/titles/bhtmux2/tmux-2/\">book on tmux</a> too) and of course the environment and language that ties it all together for me - <a href=\"https://www.gnu.org/software/bash/\">Bash</a>.</p>\n<p>The lockdown has afforded me time to read more, and I need to embrace that and work out how I can keep that momentum up. I want to tip the balance over from always having my fingers on the keyboard towards stepping away from the keyboard to read, reflect and consolidate my learning.</p>\n<hr />\n<p>Update 02 Feb 2021: I've started digging deeper into <code>fzf</code> - see <a href=\"https://qmacro.org/autodidactics/2021/02/02/fzf-the-basics-1-layout/\">fzf - the basics part 1 - layout</a> and <a href=\"https://qmacro.org/autodidactics/2021/02/07/fzf-the-basics-2-search-results/\">fzf - the basics part 2 - search results</a> over on my <a href=\"https://qmacro.org/autodidactics/\">Autodidactics</a> blog.</p>\n",
      "date_published": "2021-02-01T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/2021-01-26-columnar-layout-with-awk/",
      "url": "https://qmacro.org/blog/posts/2021-01-26-columnar-layout-with-awk/",
      "title": "Columnar layout with AWK",
      "content_html": "<p><em>Here's a breakdown of a simple AWK script I wrote to format values into neatly aligned columns</em></p>\n<p>(Jump to the end for a couple of updates, thanks gioele and oh5nxo!)</p>\n<p>I'm organising my GitHub repositories locally by creating a directory structure representing the different GitHub servers that I use and the orgs and users that I have access to, with symbolic links at the ends of these structures pointing to where I've cloned the actual repositories.</p>\n<p>Here's an example of what I started out with:</p>\n<pre><code>; find ~/gh -type l\n/Users/dja/gh/github.tools.sap/developer-relations/advocates-team-general\n/Users/dja/gh/github.com/SAP-samples/teched2020-developer-keynote\n/Users/dja/gh/github.com/qmacro-org/auto-tweeter\n</code></pre>\n<p>and what I wanted to end up with (you can see the invocation of the script here too):</p>\n<pre><code>; find ~/gh -type l | awk -F/ -vCOLS=5,6,7 -f ~/.dotfiles/scripts/cols.awk\ngithub.tools.sap developer-relations advocates-team-general\ngithub.com       SAP-samples         teched2020-developer-keynote\ngithub.com       qmacro-org          auto-tweeter\n</code></pre>\n<p>In other words, I wanted to select columns from the output and have them printed neatly and aligned. Don't ask me why, I guess it's just some form of OCD.</p>\n<p><a name=\"homage\"></a><strong>Homage</strong></p>\n<p>Anyway, I decided to write this in AWK, partly because I don't know AWK that well, but mostly as a meditation on the early days of Unix and a homage to <a href=\"https://en.wikipedia.org/wiki/Brian_Kernighan\">Brian Kernighan</a>. Talking of homages, I've also decided to share this script by describing it line by line, in homage to <a href=\"https://en.wikipedia.org/wiki/Randal_L._Schwartz\">Randal L Schwartz</a>, that maverick hero that I learned a great deal from in the Perl world.</p>\n<p>Randal <a href=\"http://www.stonehenge.com/merlyn/columns.html\">wrote columns for magazines</a>, each time listing and describing a Perl script he'd written, line by line. I learned so much from Randal and enjoyed the format, so I thought I'd reproduce it here.</p>\n<p>Let's start with the script, in full, courtesy of GitHub's embeddable Gist mechanism, which, incidentally, I created from the command line using <a href=\"https://github.com/cli/cli\">GitHub's CLI <code>gh</code></a>, like this:</p>\n<pre><code>; gh gist create --public scripts/cols.awk\n</code></pre>\n<p>I subsequently edited it too (there are now <a href=\"https://gist.github.com/qmacro/c84f5a17dc4740dc2defa6a913cd3c2c/revisions\">multiple revisions</a>) with:</p>\n<pre><code>; gh gist edit c84f5a17dc4740dc2defa6a913cd3c2c\n</code></pre>\n<p>OK, so here's the entire script.</p>\n<script src=\"https://gist.github.com/qmacro/c84f5a17dc4740dc2defa6a913cd3c2c.js\"></script>\n<p>Remember that an AWK scripts are generally data driven, in that you describe patterns and then what to do when those patterns are matched. This is described nicely in the <a href=\"https://www.gnu.org/software/gawk/manual/html_node/Getting-Started.html#Getting-Started\">Getting Started with <code>awk</code></a> section of the GNU AWK manual. The approach is &lt;pattern&gt; &lt;action&gt;, where the actions are within a <code>{...}</code> block. In this script, there are two special (and common) patterns used: <code>BEGIN</code> and <code>END</code>, i.e. before and after all lines have been processed. There's an &lt;action&gt; block in the middle which has no pattern; that means it's called for each and every line in the input. There's also an &lt;action&gt; block with a specific pattern, which we'll look at shortly.</p>\n<p><strong>The invocation</strong></p>\n<p>Note the invocation earlier looks like this:</p>\n<pre><code>awk -F/ -vCOLS=5,6,7 -f ~/.dotfiles/scripts/cols.awk\n</code></pre>\n<p>Here are what the options do:</p>\n<ul>\n<li><code>-F/</code> says that the input field separator is the <code>/</code> character</li>\n<li><code>-vCOLS=5,6,7</code> sets the value <code>5,6,7</code> for the variable <code>COLS</code></li>\n<li><code>-f &lt;script&gt;</code> tells AWK where to find the script</li>\n</ul>\n<p>OK, let's start digging in.</p>\n<p><strong>The <code>BEGIN</code> pattern</strong></p>\n<p>Lines 7-9 just make sure that the optional <code>GAP</code> variable, if not explicitly set (using a <code>-v</code> option in the invocation) is set to 1. That's how many spaces we want between each column. If we had wanted a value other than the default here, an extra option like this would be required, for example <code>-vGAP=2</code>.</p>\n<p><strong>The <code>NR == 1</code> pattern</strong></p>\n<p>The action in this block is executed only on one occasion - when the value of <code>NR</code> is <code>1</code>.</p>\n<p><code>NR</code> is a special AWK variable that represents the record number, i.e. the value is <code>1</code> for the first record, <code>2</code> for the second, and so on. Note that there's also <code>FNR</code> (file record number) which comes in handy when you're processing multiple input files. So the &lt;action&gt; block related to this <code>NR == 1</code> pattern is only executed once, when processing the first record in the input.</p>\n<p>This &lt;action&gt; block, specifically lines 18-24, deal with the value for the <code>COLS</code> variable. If it's been set (as in our invocation: <code>-vCOLS=5,6,7</code>) it splits out the column numbers (5,6 and 7 here) into an array <code>fieldlist</code>. If it's not been set, then the default should be all columns, which are put into the <code>fieldlist</code> array using the loop in lines 21-23. Note that <code>NF</code> is another special variable, the value of which tells us the number of fields in the current record.</p>\n<p><strong>The default pattern</strong></p>\n<p>Lines 31-36 represent the action for the default pattern, i.e. this is executed for each line in the input. That includes even the first record, although we've done some processing for the first record in the &lt;action&gt; block for the <code>NR == 1</code> pattern already. That's because <em>all</em> patterns are tested, in sequence, unless an action invokes an explicit <code>next</code> to skip to the next input record (see update #2 at the end of this post for the attribution for this info).</p>\n<p>The script has to work out what the longest word in each column is, and for that it needs to read through the entire input. I think perhaps there may be better ways of doing this, but here's what I did.</p>\n<p>Because this script needs two passes over the input, we store the current record in an array called <code>records</code> in line 32. Worthy of note here is that each field in a record is represented by its positional variable i.e. <code>$1</code>, <code>$2</code>, and so on, and <code>$0</code> represents the entire record. In lines 33-35 we build up an array <code>fieldlengths</code> of the longest field by position. Arguably we only really need to remember the longest lengths of the fields in <code>fieldlist</code>, but hey.</p>\n<p><strong>The <code>END</code> pattern</strong></p>\n<p>Lines 40-49 represent the action for the special <code>END</code> pattern, i.e. once the records have been processed (once). At this stage we have the longest lengths for each of the fields (columns), and now we just need to go through the input again, which we have in the <code>records</code> array.</p>\n<p>In line 42 we use the <code>split</code> function to split out the record we're processing into an array called <code>fields</code>:</p>\n<pre><code>split(records[record], fields, FS)\n</code></pre>\n<p>The third argument supplied to this call is <code>FS</code>, which is another special variable representing the field separator for this execution. Remember the <code>-F/</code> option in the invocation, shown earlier? In this case, the value of <code>FS</code> is also therefore <code>/</code>. If the field separator is different (the default is whitespace) then the value of <code>FS</code> will be different too.</p>\n<p>Then in lines 43-46 we start printing out each chosen field (remember, the chosen ones are in <code>fieldlist</code>). The <code>printf</code> call in line 45 is special, let's break that down here:</p>\n<pre><code>printf &quot;%*-s&quot;, fieldlengths[f] + GAP, fields[f]\n</code></pre>\n<p>Like other flavours of <code>printf</code>, this one also takes a pattern and one or more variables to substitute into that pattern. The pattern here is for a single variable, and is <code>%*-s</code>. This means that the variable to print is a string (basic form is <code>%s</code>), which should be padded out, left justified (<code>-</code>) by a value also to be supplied as a variable (<code>*</code>).</p>\n<p>So we need to supply two variables, the width to which the variable value should be padded, and the variable itself. And that's what is supplied. First, we have <code>fieldlengths[f] + GAP</code>, which works out to be the longest length for that field (column), plus zero or more spaces as defined in <code>GAP</code>. Then we have the variable that we want printed, i.e. <code>fields[f]</code>.</p>\n<p>Noting that <code>printf</code> won't print a newline unless it's explicitly given (as <code>\\n</code>), this works well because then the consecutive fields are printed on the same line. Line 47 takes care of printing a newline when all the fields are output for that record.</p>\n<p>And that's it. As the tagline for this blog says, I reserve the right to be wrong. I'm not a proficient AWK scripter, but this works for me.</p>\n<p>Happy scripting!</p>\n<p><em>Update #1, later the same day: Over on Lobsters, the user <a href=\"https://gioele.io/\">gioele</a> <a href=\"https://lobste.rs/s/r5ezxh/columnar_layout_with_awk#c_8cunpb\">contributed a pipeline version</a>, which also helps me in a different area (small pieces loosely joined) of the same Unix meditation: <code>find ~/gh -type | cut -d/ -f5,6,7 | column -s/ -t</code>. Thanks gioele!</em></p>\n<p><em>Update #2, even later the same day: Over on Reddit, the user <a href=\"https://www.reddit.com/user/oh5nxo/\">oh5nxo</a> puts me right; in an earlier version of this script (and this blog post) I'd put the lines of code that are now in the <code>NR == 1</code> &lt;action&gt; block inside the main (default) &lt;action&gt; block, as I'd mistakenly thought that I'd have to otherwise repeat some code. That wasn't the case. Thanks for <a href=\"https://www.reddit.com/r/commandline/comments/l5ivt7/columnar_layout_with_awk/gkuxhx0/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3\">sharing your knowledge</a>, oh5nxo! I've updated the script and this post to reflect that.</em></p>\n",
      "date_published": "2021-01-26T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/fourthpost/",
      "url": "https://qmacro.org/blog/posts/fourthpost/",
      "title": "This is my fourth post.",
      "content_html": "<p>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.</p>\n<p>Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.</p>\n<h2 id=\"section-header\" tabindex=\"-1\">Section Header <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/fourthpost/#section-header\" aria-hidden=\"true\">#</a></h2>\n<p>Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.</p>\n",
      "date_published": "2018-09-30T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/thirdpost/",
      "url": "https://qmacro.org/blog/posts/thirdpost/",
      "title": "This is my third post.",
      "content_html": "<p>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.</p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"highlight-line\"><span class=\"token comment\">// this is a command</span></span><br /><span class=\"highlight-line\"><span class=\"token keyword\">function</span> <span class=\"token function\">myCommand</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span></span><br /><ins class=\"highlight-line highlight-line-add\">\t<span class=\"token keyword\">let</span> counter <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span></ins><br /><span class=\"highlight-line\"></span><br /><del class=\"highlight-line highlight-line-remove\">\tcounter<span class=\"token operator\">++</span><span class=\"token punctuation\">;</span></del><br /><span class=\"highlight-line\"></span><br /><span class=\"highlight-line\"><span class=\"token punctuation\">}</span></span><br /><span class=\"highlight-line\"></span><br /><span class=\"highlight-line\"><span class=\"token comment\">// Test with a line break above this line.</span></span><br /><span class=\"highlight-line\">console<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Test'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></span></code></pre>\n<p>Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.</p>\n<h2 id=\"section-header\" tabindex=\"-1\">Section Header <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/thirdpost/#section-header\" aria-hidden=\"true\">#</a></h2>\n<p>Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.</p>\n",
      "date_published": "2018-08-24T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/secondpost/",
      "url": "https://qmacro.org/blog/posts/secondpost/",
      "title": "This is my second post.",
      "content_html": "<p>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.</p>\n<h2 id=\"section-header\" tabindex=\"-1\">Section Header <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/secondpost/#section-header\" aria-hidden=\"true\">#</a></h2>\n<p><a href=\"https://qmacro.org/blog/posts/firstpost/\">First post</a><br />\n<a href=\"https://qmacro.org/blog/posts/thirdpost/\">Third post</a></p>\n<p>Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.</p>\n<p>Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.</p>\n",
      "date_published": "2018-07-04T00:00:00Z"
    },{
      "id": "https://qmacro.org/blog/posts/firstpost/",
      "url": "https://qmacro.org/blog/posts/firstpost/",
      "title": "This is my first post.",
      "content_html": "<p>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.</p>\n<p>Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.</p>\n<h2 id=\"some-code\" tabindex=\"-1\">Some code <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/firstpost/#some-code\" aria-hidden=\"true\">#</a></h2>\n<p>Here is some code</p>\n<pre><code>\n#!/usr/bin/env bash\n\n# Create new Bash shell script - newscript\n\ndeclare script=$1\nif [[ ! -f &quot;$script&quot; ]]; then\n  touch &quot;$script&quot;\n  chmod +x &quot;$script&quot;\nfi\n\n&quot;$EDITOR&quot; &quot;$script&quot;\n</code></pre>\n<h2 id=\"section-header\" tabindex=\"-1\">Section Header <a class=\"direct-link\" href=\"https://qmacro.org/blog/posts/firstpost/#section-header\" aria-hidden=\"true\">#</a></h2>\n<p>Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.</p>\n<pre class=\"language-text\"><code class=\"language-text\"><span class=\"highlight-line\">// this is a command</span><br /><span class=\"highlight-line\">function myCommand() {</span><br /><mark class=\"highlight-line highlight-line-active\">\tlet counter = 0;</mark><br /><mark class=\"highlight-line highlight-line-active\">\tcounter++;</mark><br /><span class=\"highlight-line\">}</span><br /><span class=\"highlight-line\"></span><br /><span class=\"highlight-line\">// Test with a line break above this line.</span><br /><span class=\"highlight-line\">console.log('Test');</span></code></pre>\n",
      "date_published": "2018-05-01T00:00:00Z"
    }
  ]
}
